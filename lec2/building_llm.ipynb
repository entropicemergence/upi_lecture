{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3984a680",
   "metadata": {},
   "source": [
    "# Building a Language Model from Scratch\n",
    "\n",
    "This notebook walks you through the process of building and training a small transformer-based language model from scratch using PyTorch. We will cover the following steps:\n",
    "1.  **Setting up the Environment**: Importing necessary libraries.\n",
    "2.  **Data Preparation**: Downloading and preprocessing the TinyStories dataset.\n",
    "3.  **Tokenization**: Creating a custom tokenizer to convert text into numerical representations.\n",
    "4.  **Model Definition**: Building the components of a transformer model (Positional Encoding, Multi-Head Attention, Feed-Forward Networks) and assembling them into a language model.\n",
    "5.  **Training**: Setting up the training loop to train the model on our dataset.\n",
    "6.  **Inference**: Using the trained model to generate new text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641a8572",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6c8a9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9640326",
   "metadata": {},
   "source": [
    "## 1. Setting up the Environment\n",
    "\n",
    "First, let's import the necessary libraries that we'll use throughout the notebook. These include `os` for interacting with the file system, `requests` for downloading data from the internet, `zipfile` for handling compressed files, `re` for regular expressions (useful for tokenization), and `Counter` for counting word frequencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f1a75fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, requests, zipfile, re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655f24db",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "\n",
    "### Downloading the Dataset\n",
    "We'll be using the \"TinyStories\" dataset, which is a collection of short stories generated by GPT-3.5 and GPT-4 that are simple enough for a small model to learn from. The code below defines the URL for the dataset and a function to download it if it's not already present in our local directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae4a80d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_url = \"https://github.com/entropicemergence/tiny_llm_server/releases/download/v0.1.0/TinyStoriesV2-GPT4-small.zip\"\n",
    "datset_folder = \"stories\"\n",
    "os.makedirs(datset_folder, exist_ok=True)\n",
    "\n",
    "def download_dataset(url):\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    filepath = os.path.join(datset_folder, filename)\n",
    "    if not os.path.exists(filepath):\n",
    "        response = requests.get(url)\n",
    "        with open(filepath, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "download_dataset(dataset_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b611b4",
   "metadata": {},
   "source": [
    "### Extracting and Loading the Data\n",
    "\n",
    "After downloading the compressed file, we need to extract its contents. The following cell unzips the file and then reads the entire text from the `.txt` file into a single string variable called `stories_text`. This variable will be our entire corpus for training the language model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903a11fb",
   "metadata": {},
   "source": [
    "## 3. Tokenization\n",
    "\n",
    "Tokenization is the process of converting a sequence of text into a sequence of numerical IDs that our model can understand. We'll create a custom `Tokenizer` class to handle this.\n",
    "\n",
    "Our tokenizer will use a hybrid approach:\n",
    "*   **Word-level Tokenization**: It will map the most common words in our vocabulary to unique integer IDs.\n",
    "*   **Character-level Tokenization**: For words that are not in our vocabulary (out-of-vocabulary or OOV words), it will break them down into individual characters and map those to IDs.\n",
    "\n",
    "This hybrid strategy allows us to handle any word we encounter while keeping the vocabulary size manageable.\n",
    "\n",
    "The tokenizer also includes several **special tokens**:\n",
    "*   `<PAD>`: Padding token, used to make all sequences in a batch the same length.\n",
    "*   `<UNKNOWN>`: Used for characters that are not in our character vocabulary.\n",
    "*   `<BOS>`: \"Beginning of Sequence\" token, marks the start of a text sequence.\n",
    "*   `<EOS>`: \"End of Sequence\" token, marks the end of a text sequence.\n",
    "*   `<CHAR_START>` / `<CHAR_END>`: These tokens signal the beginning and end of a sequence of characters representing an OOV word.\n",
    "\n",
    "The cell below defines the `Tokenizer` class, builds the vocabulary from our `stories_text`, and then demonstrates how to encode a sample sentence into token IDs and decode it back into text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "538a9d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"lec2/stories/TinyStoriesV2-GPT4-small.zip\"\n",
    "with zipfile.ZipFile(filepath, \"r\") as zip_ref:\n",
    "    zip_ref.extractall(datset_folder)\n",
    "with open(\"lec2/stories/TinyStoriesV2-GPT4-small.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    stories_text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b1e36a",
   "metadata": {
    "vscode": {
     "languageId": "markdown\n"
    }
   },
   "source": [
    "Now, we'll import the core `PyTorch` libraries. `torch` is the main library, `torch.nn` provides building blocks for neural networks (like layers, activation functions, etc.), and `torch.optim` contains optimization algorithms like Adam.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e12a443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22493387\n",
      "{'<PAD>': 0, '<UNKNOWN>': 1, '<BOS>': 2, '<EOS>': 3, '<CHAR_START>': 4, '<CHAR_END>': 5, '.': 4056, 'the': 7, ',': 4009, 'and': 9, 'a': 4051, 'to': 11, '\"': 4032, 'was': 13, 'they': 14, 'he': 15, 'it': 16, 'she': 17, 'said': 18, 'with': 19, 'day': 20, 'tim': 21, 'his': 22, 'her': 23, \"'\": 4050, 'in': 25, '!': 4025, 'you': 27, 'big': 28, 'but': 29, 'one': 30, 'had': 31, 'that': 32, 'not': 33, 'i': 4035, 'mom': 35, 'on': 36, 'happy': 37, 'of': 38, 'very': 39, 'saw': 40, 'lily': 41, 's': 4029, 'play': 43, 'little': 44, 'so': 45, 'there': 46, 'tom': 47, 'for': 48, 'time': 49, 'named': 50, 'were': 51, 'wanted': 52, 'friends': 53, 'all': 54, 'went': 55, 'once': 56, 'bird': 57, 'is': 58, 'upon': 59, 'can': 60, 'together': 61, 'at': 62, 'fun': 63, '?': 4055, 'help': 65, 'be': 66, 'dog': 67, 'their': 68, 'cat': 69, 'girl': 70, 'played': 71, 'are': 72, 'up': 73, 'did': 74, 'him': 75, 't': 4030, 'ball': 77, 'sue': 78, 'too': 79, 'could': 80, 'them': 81, 'sad': 82, 'boy': 83, 'tree': 84, 'max': 85, 'have': 86, 'loved': 87, 'found': 88, 'looked': 89, 'from': 90, 'then': 91, 'when': 92, 'friend': 93, 'go': 94, 'back': 95, 'ben': 96, 'we': 97, 'sam': 98, 'what': 99, 'toy': 100, 'park': 101, 'would': 102, 'liked': 103, 'like': 104, 'felt': 105, 'asked': 106, 'spot': 107, 'came': 108, 'started': 109, 'do': 110, 'new': 111, 'make': 112, 'want': 113, 'as': 114, 'smiled': 115, 'see': 116, 'small': 117, 'good': 118, 'away': 119, 'out': 120, 'made': 121, 'something': 122, 'playing': 123, 'home': 124, 'let': 125, 'find': 126, 'thought': 127, 'other': 128, 'put': 129, 'down': 130, 'again': 131, 'toys': 132, 'ran': 133, 'scared': 134, 'took': 135, 'mia': 136, 'me': 137, 'around': 138, 'this': 139, 'will': 140, 'house': 141, 'my': 142, 'box': 143, 'lived': 144, 'things': 145, 'your': 146, 'some': 147, 'an': 148, 'car': 149, 'lucy': 150, 'tried': 151, 'many': 152, 'no': 153, 'look': 154, 'got': 155, 'who': 156, 'man': 157, 'about': 158, 'says': 159, 'every': 160, 'red': 161, 'knew': 162, 'get': 163, 'know': 164, 'inside': 165, 'more': 166, 'best': 167, 'yes': 168, 'became': 169, 'dad': 170, 'because': 171, 'learned': 172, 'after': 173, 'always': 174, 'both': 175, 'water': 176, 'outside': 177, 'into': 178, 'laughed': 179, 'decided': 180, 'room': 181, 'don': 182, 'just': 183, 'now': 184, 'excited': 185, 'how': 186, 'thank': 187, 'told': 188, 'nice': 189, 'long': 190, 'bob': 191, 'each': 192, 'pretty': 193, 'amy': 194, 'gave': 195, 'anna': 196, 'sorry': 197, 'much': 198, 'eat': 199, 'sun': 200, 'old': 201, 'take': 202, 'walked': 203, 'if': 204, 'fish': 205, 'fast': 206, 'animals': 207, 'sky': 208, 'come': 209, 'happened': 210, 'food': 211, 'idea': 212, 'bear': 213, 'mommy': 214, 'kind': 215, 'its': 216, 'didn': 217, 'flew': 218, 'everyone': 219, 'share': 220, 'careful': 221, 'heard': 222, 'better': 223, 'special': 224, 'cake': 225, 'lots': 226, 'blue': 227, 'near': 228, 'while': 229, 'never': 230, 'anymore': 231, 'even': 232, 'people': 233, 'rock': 234, 'by': 235, 'suddenly': 236, 'am': 237, 'soon': 238, 'under': 239, 'over': 240, 'safe': 241, 'run': 242, 'high': 243, 'sara': 244, 'ground': 245, 'shiny': 246, 'hard': 247, 'opened': 248, 'bad': 249, 'picked': 250, 'end': 251, 'surprised': 252, 'lot': 253, 'm': 4021, 'give': 255, 'unexpected': 256, 'feel': 257, 'proud': 258, 'thanked': 259, 'door': 260, 'sally': 261, 'bug': 262, 'ever': 263, 'garden': 264, 'why': 265, 'still': 266, 'hugged': 267, 'okay': 268, 'way': 269, 'clean': 270, 'hurt': 271, 'next': 272, 'jumped': 273, 'family': 274, 'until': 275, 'say': 276, 'kitty': 277, 'try': 278, 'forest': 279, 'loud': 280, 'great': 281, 'fluffy': 282, 'fly': 283, 'walk': 284, 'helped': 285, 'love': 286, 'happily': 287, 'should': 288, 'story': 289, '-': 4006, 'show': 291, 'beautiful': 292, 'need': 293, 'place': 294, 'kids': 295, 'showed': 296, 'called': 297, 'bunny': 298, 'please': 299, 'strong': 300, 'being': 301, 'boat': 302, 'town': 303, 'off': 304, 'where': 305, 'listen': 306, 'magic': 307, 'stopped': 308, 'yummy': 309, 'angry': 310, 'frog': 311, 'met': 312, 'mouse': 313, 'open': 314, 'soft': 315, 'wind': 316, 'ate': 317, 'lost': 318, 'slide': 319, 'rabbit': 320, 'couldn': 321, 'or': 322, 'two': 323, 'game': 324, 'fell': 325, 'flowers': 326, 'before': 327, 'brave': 328, 'truck': 329, 'warm': 330, 'green': 331, 'sunny': 332, 'hat': 333, 'noise': 334, 'also': 335, 'jack': 336, 'hi': 337, 'nodded': 338, 'hello': 339, 'having': 340, 'sat': 341, 'smile': 342, 'keep': 343, 'kept': 344, 'hole': 345, 'looking': 346, 'jump': 347, 'watch': 348, 'hands': 349, 'store': 350, 'used': 351, 'birds': 352, 'here': 353, 'funny': 354, '”': 4060, 'catch': 356, 'going': 357, 'tired': 358, 'stick': 359, 'maybe': 360, 'ice': 361, 'bed': 362, 'party': 363, 'has': 364, 'pond': 365, 'kitchen': 366, 'squirrel': 367, 'face': 368, 'our': 369, 'stop': 370, 'moral': 371, 'hand': 372, 'worked': 373, 'hug': 374, 'others': 375, 'book': 376, 'eyes': 377, 'shared': 378, 'favorite': 379, 'talk': 380, 'right': 381, 'finally': 382, 'first': 383, 'mean': 384, 'making': 385, 'timmy': 386, 'cold': 387, 'use': 388, 'work': 389, 'curious': 390, 'yard': 391, 'flower': 392, 'top': 393, 'important': 394, 'doll': 395, 'head': 396, 'us': 397, 'stay': 398, 'began': 399, 'watched': 400, 'wow': 401, 'wait': 402, 'rain': 403, 'sure': 404, 'thing': 405, 'tiny': 406, 'hill': 407, 'race': 408, 'learn': 409, 'move': 410, 'agreed': 411, 'surprise': 412, 'floor': 413, 'lady': 414, 'john': 415, 'tail': 416, 'cream': 417, 'climb': 418, 'night': 419, 'yellow': 420, 'worry': 421, 'jane': 422, 'needed': 423, 'grass': 424, 'leaves': 425, 'picture': 426, 'trees': 427, 'table': 428, ':': 4027, 'dark': 430, 'walking': 431, 'different': 432, 'turned': 433, 'mr': 434, 'hungry': 435, 'cars': 436, 'voice': 437, 'colors': 438, 'bobo': 439, 'than': 440, 'fix': 441, 'behind': 442, 'bright': 443, 'closer': 444, 'broken': 445, 'sometimes': 446, 'cried': 447, 'friendly': 448, 'ready': 449, 'far': 450, 'dress': 451, 'wet': 452, 'tall': 453, 'hot': 454, 'full': 455, 'lila': 456, 'bag': 457, 'another': 458, 'climbed': 459, 'goodbye': 460, 'dance': 461, 'apple': 462, 'gone': 463, 'feeling': 464, 'blocks': 465, 'read': 466, 'does': 467, 'swing': 468, 'touch': 469, 'paper': 470, 'sweet': 471, 'running': 472, 'only': 473, 'helping': 474, 'lion': 475, 'reached': 476, 'mum': 477, 'listened': 478, 'owl': 479, 'sing': 480, 'cry': 481, 'baby': 482, 'laugh': 483, 'pushed': 484, 'think': 485, 'train': 486, 'done': 487, 'wise': 488, 'care': 489, 'duck': 490, 'glad': 491, 'trying': 492, 'stuck': 493, 'mess': 494, 'through': 495, 'wished': 496, 'sees': 497, 'pick': 498, 'been': 499, 'looks': 500, 'doing': 501, 'hide': 502, 'shouted': 503, 'close': 504, 'reach': 505, 'moment': 506, 'coming': 507, 'scary': 508, 'oh': 509, 'buy': 510, 'sit': 511, 'blew': 512, 'tell': 513, 'seen': 514, 'explore': 515, 'dirty': 516, 'held': 517, 'nest': 518, 'everywhere': 519, 'sound': 520, 'bigger': 521, 'replied': 522, 'left': 523, 'hear': 524, 'colorful': 525, 'grandma': 526, 'pulled': 527, 'grabbed': 528, 'turns': 529, 'remembered': 530, 'pink': 531, 'name': 532, 'three': 533, 'broke': 534, 'daisy': 535, 'hopped': 536, 'ask': 537, 'stayed': 538, 'clapped': 539, 'bit': 540, 'today': 541, 'must': 542, 'rest': 543, 'games': 544, 'window': 545, 'hair': 546, 'job': 547, 'waited': 548, 're': 549, 'cool': 550, 'fin': 551, 'well': 552, 'noticed': 553, 'swam': 554, 'anything': 555, 'sand': 556, 'pictures': 557, 'woods': 558, 'lesson': 559, 'brown': 560, 'waved': 561, 'buddy': 562, 'enjoyed': 563, 'song': 564, 'adventure': 565, 'alone': 566, 'likes': 567, 'sea': 568, 'quickly': 569, 'white': 570, 'sleep': 571, 'threw': 572, 'air': 573, 'chair': 574, 'books': 575, 'heavy': 576, 'followed': 577, 'butterfly': 578, 'ant': 579, 'cookies': 580, 'mouth': 581, 'light': 582, 'world': 583, 'wrong': 584, 'clothes': 585, 'castle': 586, 'sarah': 587, 'sharing': 588, 'paint': 589, 'billy': 590, 'leaf': 591, 'cut': 592, 'draw': 593, 'round': 594, 'tommy': 595, 'fire': 596, 'later': 597, 'makes': 598, 'ok': 599, 'brought': 600, 'wall': 601, 'own': 602, 'wants': 603, 'himself': 604, 'swim': 605, 'daddy': 606, 'ride': 607, 'wear': 608, 'danced': 609, 'tower': 610, 'key': 611, 'grow': 612, 'faster': 613, 'won': 614, 'deep': 615, 'fox': 616, 'join': 617, 'talked': 618, 'really': 619, 'forgot': 620, 'king': 621, 'wish': 622, 'beach': 623, 'bee': 624, 'brother': 625, 'finished': 626, 'someone': 627, 'branch': 628, 'everything': 629, 'tight': 630, 'dinner': 631, 'quiet': 632, 'dry': 633, 'might': 634, 'river': 635, 'monster': 636, 'fruit': 637, 'field': 638, 'pretend': 639, 'any': 640, 'plan': 641, 'farm': 642, 'hit': 643, 'bottle': 644, 'real': 645, 'sister': 646, 'balloon': 647, 'teddy': 648, 'welcome': 649, 'silly': 650, 'carefully': 651, 'gift': 652, 'star': 653, 'rocks': 654, 'worried': 655, 'sign': 656, 'fairy': 657, 'mark': 658, 'mimi': 659, 'smart': 660, 'eating': 661, 'filled': 662, 'win': 663, 'soup': 664, 'set': 665, 'promised': 666, 'dragon': 667, 'parents': 668, 'bowl': 669, 'mud': 670, 'fight': 671, 'pieces': 672, 'fall': 673, 'children': 674, 'else': 675, 'joe': 676, 'bite': 677, 'instead': 678, 'cloud': 679, 'sang': 680, 'continued': 681, 'flying': 682, 'talking': 683, 'button': 684, 'kite': 685, 'without': 686, 'snake': 687, 'smiles': 688, 'understand': 689, 'realized': 690, 'mother': 691, 'leave': 692, 'words': 693, 'amazing': 694, 'orange': 695, 'laughing': 696, 'build': 697, 'wagged': 698, 'enough': 699, 'sitting': 700, 'jar': 701, 'moved': 702, 'remember': 703, 'closed': 704, 'candy': 705, 'dangerous': 706, 'shining': 707, 'side': 708, 'molly': 709, 'rope': 710, 'strange': 711, 'barked': 712, 'tasty': 713, 'cup': 714, 'road': 715, 'perfect': 716, 'turn': 717, 'crying': 718, 'monkey': 719, 'hold': 720, 'crab': 721, 'living': 722, 'higher': 723, 'farmer': 724, 'slow': 725, 'lonely': 726, 'visit': 727, 'owner': 728, 'shoes': 729, 'morning': 730, 'dream': 731, 'school': 732, 'delicious': 733, 'bike': 734, 'years': 735, 'sack': 736, 'push': 737, 'black': 738, 'purple': 739, 'dolls': 740, 'swings': 741, 'rolled': 742, 'animal': 743, 'drink': 744, 'stone': 745, 'secret': 746, 'mine': 747, 'puddle': 748, 'getting': 749, 'cow': 750, 'break': 751, 'asks': 752, 'shop': 753, 'mad': 754, 'lake': 755, 'mama': 756, 'caught': 757, 'nap': 758, 'money': 759, 'runs': 760, 'wore': 761, 'hoped': 762, 'bella': 763, 'piece': 764, 'singing': 765, 'music': 766, 'same': 767, 'whiskers': 768, 'leo': 769, 'pot': 770, 'letter': 771, 'herself': 772, 'wings': 773, 'apples': 774, 'days': 775, 'cute': 776, 'plane': 777, 'plant': 778, 'gentle': 779, 'bow': 780, 'teacher': 781, 'balls': 782, 'dropped': 783, 'fit': 784, 'promise': 785, 'joy': 786, 'birthday': 787, 'course': 788, 'forever': 789, 'hiding': 790, 'woke': 791, 'naughty': 792, 'cookie': 793, 'treat': 794, 'most': 795, 'slowly': 796, 'milk': 797, 'doctor': 798, 'treasure': 799, 'thinks': 800, 'start': 801, 'pull': 802, 'pile': 803, 'true': 804, 'ugly': 805, 'juice': 806, 'elephant': 807, 'nose': 808, 'fence': 809, 'bush': 810, 'blanket': 811, 'grew': 812, 'follow': 813, 'snack': 814, 'shapes': 815, 'lunch': 816, 'momo': 817, 'sharp': 818, 'bench': 819, 'bring': 820, 'young': 821, 'll': 822, 'believe': 823, 'pet': 824, 'bone': 825, 'able': 826, 'hid': 827, 'late': 828, 'street': 829, 'heart': 830, 'emma': 831, 'bread': 832, 'coin': 833, 'throw': 834, 'magical': 835, 'touched': 836, 'busy': 837, 'shook': 838, 'twins': 839, 'gray': 840, 'arrived': 841, 'cheered': 842, 'dirt': 843, 'feels': 844, 'robot': 845, 'taking': 846, 'grumpy': 847, 'shy': 848, 'amazed': 849, 'lucky': 850, 'land': 851, 'few': 852, 'village': 853, 'whistle': 854, 'snow': 855, 'stories': 856, 'storm': 857, 'fixed': 858, 'sounds': 859, 'rude': 860, 'wonderful': 861, 'turtle': 862, 'kitten': 863, 'moon': 864, 'rocket': 865, 'towards': 866, 'wave': 867, 'coat': 868, 'huge': 869, 'pocket': 870, 'free': 871, 'enjoy': 872, 'fred': 873, 'songs': 874, 'string': 875, 'zoom': 876, 'cheese': 877, 'bucket': 878, 'wide': 879, 'bounce': 880, 'remy': 881, 'teach': 882, 'princess': 883, 'rainbow': 884, 'treats': 885, 'driver': 886, 'holding': 887, 'puppy': 888, 'upset': 889, 'hop': 890, 'fighting': 891, 'bell': 892, 'trip': 893, 'bugs': 894, 'patient': 895, 'life': 896, 'giant': 897, 'honey': 898, 'hugs': 899, 'spoon': 900, 'bus': 901, 'wild': 902, 'noises': 903, 'smelly': 904, 'snowman': 905, 'smooth': 906, 'hope': 907, 'cook': 908, 'these': 909, 'landed': 910, 'basket': 911, 'fair': 912, 'ollie': 913, 'seek': 914, 'dogs': 915, 'cave': 916, 'sour': 917, 'cloth': 918, 'splash': 919, 'path': 920, 'happen': 921, 'cannot': 922, 'seat': 923, 'missed': 924, 'jill': 925, 'stars': 926, 'beak': 927, 'mummy': 928, 'fur': 929, 'drove': 930, 'oven': 931, 'lisa': 932, 'drive': 933, 'shelf': 934, 'year': 935, 'shake': 936, 'ducks': 937, 'taste': 938, 'honest': 939, 'sick': 940, 'ella': 941, 'pig': 942, 'across': 943, 'egg': 944, 'roll': 945, 'ann': 946, 'hey': 947, 'fancy': 948, 'adventures': 949, 'leg': 950, 'bought': 951, 'helpful': 952, 'wash': 953, 'confused': 954, 'lay': 955, 'poppy': 956, 'grateful': 957, 'tea': 958, 'line': 959, 'change': 960, 'shell': 961, 'comes': 962, 'doggy': 963, 'working': 964, 'healthy': 965, 'ducky': 966, 'count': 967, 'rose': 968, 'stepped': 969, 'last': 970, 'wasn': 971, 'along': 972, 'wolf': 973, 'anyone': 974, 'easy': 975, 'sticks': 976, 'whole': 977, 'miss': 978, 'watching': 979, 'carry': 980, 'elderly': 981, 'selfish': 982, 'save': 983, 'benny': 984, 'messy': 985, 'takes': 986, 'chased': 987, 'write': 988, 'shirt': 989, 'tidy': 990, 'tape': 991, 'mary': 992, 'chirpy': 993, 'card': 994, 'ladder': 995, 'horse': 996, 'mix': 997, 'legs': 998, 'phone': 999, 'spin': 1000, 'longer': 1001, 'clever': 1002, 'forgive': 1003, 'arm': 1004, 'plants': 1005, 'band': 1006, 'spicy': 1007, 'problem': 1008, 'weak': 1009, 'faces': 1010, 'empty': 1011, 'stand': 1012, 'seemed': 1013, 'pretended': 1014, 'worm': 1015, 'cozy': 1016, 'jen': 1017, 'person': 1018, 'lock': 1019, 'sleepy': 1020, 'waiting': 1021, 'print': 1022, 'color': 1023, 'finish': 1024, 'clouds': 1025, 'saved': 1026, 'forget': 1027, 'jumping': 1028, 'luna': 1029, 'paw': 1030, 'screamed': 1031, 'tie': 1032, 'fake': 1033, 'pool': 1034, 'such': 1035, 'interesting': 1036, 'goat': 1037, 'cart': 1038, 'bobby': 1039, 'part': 1040, 'wondered': 1041, 'though': 1042, 'fruits': 1043, 'pen': 1044, 'rich': 1045, 'drank': 1046, 'plate': 1047, 'number': 1048, 'bossy': 1049, 'zip': 1050, 'brush': 1051, 'scissors': 1052, 'calm': 1053, 'furry': 1054, 'jerry': 1055, 'onto': 1056, 'machine': 1057, 'queen': 1058, 'smell': 1059, 'caterpillar': 1060, 'pillow': 1061, 'drew': 1062, 'building': 1063, 'lift': 1064, 'hidden': 1065, 'swimming': 1066, 'mail': 1067, 'grab': 1068, 'tv': 1069, 'sugar': 1070, 'crayons': 1071, 'normal': 1072, 'gives': 1073, 'team': 1074, 'child': 1075, 'sheep': 1076, 'spider': 1077, 'cleaned': 1078, 'lovely': 1079, 'nothing': 1080, 'arms': 1081, 'glue': 1082, 'front': 1083, 'wing': 1084, 'trouble': 1085, 'pizza': 1086, 'pointed': 1087, 'sail': 1088, 'cage': 1089, 'circle': 1090, 'appeared': 1091, 'meet': 1092, 'prince': 1093, 'mind': 1094, 'puts': 1095, 'gently': 1096, 'smelled': 1097, 'dizzy': 1098, 'icy': 1099, 'gate': 1100, 'note': 1101, 'backyard': 1102, 'rex': 1103, 'smiling': 1104, 'nearby': 1105, 'couch': 1106, 'spring': 1107, 'andy': 1108, 'carrot': 1109, 'zoo': 1110, 'ashamed': 1111, 'poor': 1112, 'yelled': 1113, 'powerful': 1114, 'places': 1115, 'wand': 1116, 'glass': 1117, 'match': 1118, 'tomorrow': 1119, 'large': 1120, 'carried': 1121, 'woman': 1122, 'daughter': 1123, 'tries': 1124, 'jenny': 1125, 'ring': 1126, 'jeep': 1127, 'wearing': 1128, 'expensive': 1129, 'lazy': 1130, 'unique': 1131, 'sammy': 1132, 'lamp': 1133, 'goes': 1134, 'loves': 1135, 'ants': 1136, 'weird': 1137, 'dancing': 1138, 'ship': 1139, 'afraid': 1140, 'penny': 1141, 'joke': 1142, 'towel': 1143, 'mirror': 1144, 'matter': 1145, 'frightened': 1146, 'clay': 1147, 'drum': 1148, 'grandpa': 1149, 'stood': 1150, 'cross': 1151, 'seed': 1152, 'journey': 1153, 'puzzle': 1154, 'bridge': 1155, 'paws': 1156, 'splashed': 1157, 'lemon': 1158, 'unknown': 1159, 'wheel': 1160, 'bubbles': 1161, 'bake': 1162, 'creative': 1163, 'eventually': 1164, 'feet': 1165, 'incredible': 1166, 'dinosaur': 1167, 'pencil': 1168, 'gun': 1169, 'net': 1170, 'sandwich': 1171, 'clock': 1172, 'sock': 1173, 'teeth': 1174, 'live': 1175, 'whale': 1176, 'useful': 1177, 'dig': 1178, 'wrap': 1179, 'nervous': 1180, 'boring': 1181, 'polite': 1182, 'famous': 1183, 'peaceful': 1184, 'kiss': 1185, 'flag': 1186, 'mixed': 1187, 'knocked': 1188, 'swan': 1189, 'mysterious': 1190, 'closet': 1191, 'steak': 1192, 'cap': 1193, 'sword': 1194, 'chew': 1195, 'roar': 1196, 'chase': 1197, 'soap': 1198, 'rug': 1199, 'stupid': 1200, '3': 4038, 'protect': 1202, 'kicked': 1203, 'ancient': 1204, 'measure': 1205, 'fan': 1206, 'snacks': 1207, 'corner': 1208, 'corn': 1209, 'noisy': 1210, 'jam': 1211, 'painted': 1212, 'dead': 1213, 'branches': 1214, 'square': 1215, 'shark': 1216, 'which': 1217, 'tent': 1218, 'bath': 1219, 'emily': 1220, 'enormous': 1221, 'scarf': 1222, 'answer': 1223, 'eggs': 1224, 'shoe': 1225, 'fine': 1226, 'wagon': 1227, 'melon': 1228, 'dug': 1229, 'rough': 1230, 'moms': 1231, 'jimmy': 1232, 'ow': 1233, 'chocolate': 1234, 'barn': 1235, 'test': 1236, 'bald': 1237, 'frustrated': 1238, 'banana': 1239, 'step': 1240, 'goose': 1241, 'understood': 1242, 'foolish': 1243, 'disgusting': 1244, 'wake': 1245, 'pass': 1246, 'salad': 1247, 'medicine': 1248, 'generous': 1249, 'bitter': 1250, 'wouldn': 1251, 'wood': 1252, 'rare': 1253, 'tiger': 1254, 'neck': 1255, 'smaller': 1256, 'covered': 1257, 'call': 1258, 'difficult': 1259, 'obedient': 1260, 'tasted': 1261, 'kick': 1262, 'pain': 1263, 'market': 1264, 'dull': 1265, 'mighty': 1266, 'crown': 1267, 'standing': 1268, 'breath': 1269, 'asleep': 1270, 'bored': 1271, 'thick': 1272, 'scare': 1273, 'squash': 1274, 'boot': 1275, 'relieved': 1276, 'taught': 1277, 'whenever': 1278, 'guitar': 1279, 'mask': 1280, 'bye': 1281, 'become': 1282, 'aunt': 1283, 'jolly': 1284, 'thin': 1285, 'joined': 1286, 'terrible': 1287, 'unusual': 1288, 'moving': 1289, 'cheap': 1290, 'tough': 1291, 'jealous': 1292, 'pumpkin': 1293, 'foot': 1294, 'parrot': 1295, 'exploring': 1296, 'jungle': 1297, 'pebble': 1298, 'yourself': 1299, 'knife': 1300, 'original': 1301, 'asking': 1302, 'spoiled': 1303, 'pay': 1304, 'nuts': 1305, 'chicken': 1306, 'popcorn': 1307, 'gold': 1308, 'spread': 1309, 'prize': 1310, 'clear': 1311, 'fat': 1312, 'fill': 1313, 'chess': 1314, 'kid': 1315, 'hears': 1316, 'glow': 1317, 'space': 1318, 'ending': 1319, 'mistake': 1320, 'pie': 1321, 'seeds': 1322, 'carrots': 1323, 'vase': 1324, 'mushroom': 1325, 'hay': 1326, 'thoughtful': 1327, 'feed': 1328, 'pepper': 1329, 'fierce': 1330, 'drawing': 1331, 'thankful': 1332, 'knowing': 1333, 'clumsy': 1334, 'spent': 1335, 'embarrassed': 1336, 'troubled': 1337, 'bulb': 1338, 'uncomfortable': 1339, 'finding': 1340, 'group': 1341, 'sunflower': 1342, 'hats': 1343, 'lose': 1344, 'pasta': 1345, 'island': 1346, 'chest': 1347, 'cats': 1348, 'march': 1349, 'exciting': 1350, 'balance': 1351, 'cone': 1352, 'lifted': 1353, 'boys': 1354, 'flexible': 1355, 'suit': 1356, 'shadow': 1357, 'hairy': 1358, 'leopard': 1359, 'nut': 1360, 'potato': 1361, 'ocean': 1362, 'wrote': 1363, 'crazy': 1364, 'seal': 1365, 'adventurous': 1366, 'picks': 1367, 'body': 1368, 'muffin': 1369, 'clap': 1370, 'comfortable': 1371, 'harmless': 1372, 'act': 1373, 'behave': 1374, 'tells': 1375, 'hose': 1376, 'berries': 1377, 'peter': 1378, 'searched': 1379, 'hero': 1380, 'loyal': 1381, 'organized': 1382, 'jelly': 1383, 'turkey': 1384, 'spaghetti': 1385, 'trust': 1386, 'stubborn': 1387, 'independent': 1388, 'crayon': 1389, 'anxious': 1390, 'spray': 1391, 'practiced': 1392, 'thirsty': 1393, 'playful': 1394, 'tied': 1395, 'word': 1396, 'eager': 1397, 'dish': 1398, 'gloomy': 1399, 'using': 1400, 'raced': 1401, 'jake': 1402, 'screen': 1403, 'barber': 1404, 'dependable': 1405, 'bathroom': 1406, 'yours': 1407, 'jug': 1408, 'engine': 1409, 'escape': 1410, 'bees': 1411, 'fragile': 1412, 'deer': 1413, 'celebrate': 1414, 'passed': 1415, 'diamond': 1416, 'wheat': 1417, 'jewel': 1418, 'block': 1419, 'clown': 1420, 'mailbox': 1421, 'raven': 1422, 'needs': 1423, 'jog': 1424, 'deaf': 1425, 'stove': 1426, 'sunshine': 1427, 'attic': 1428, 'dove': 1429, 'uncle': 1430, 'horn': 1431, 'painting': 1432, 'study': 1433, 'explained': 1434, 'yarn': 1435, 'changed': 1436, 'shows': 1437, 'mountain': 1438, 'temple': 1439, 'brilliant': 1440, 'board': 1441, 'humble': 1442, 'ignorant': 1443, 'search': 1444, 'tricks': 1445, 'crawled': 1446, 'comb': 1447, 'choose': 1448, 'crawl': 1449, 'map': 1450, 'balloons': 1451, 'envelope': 1452, 'crocodile': 1453, 'ruined': 1454, 'roof': 1455, 'jim': 1456, 'cheerful': 1457, 'frame': 1458, 'sunglasses': 1459, 'scale': 1460, 'bottom': 1461, 'folder': 1462, 'knee': 1463, 'counted': 1464, 'built': 1465, 'hoop': 1466, 'raft': 1467, 'cleaning': 1468, 'spotted': 1469, 'popular': 1470, 'stones': 1471, 'pants': 1472, 'delicate': 1473, 'waves': 1474, 'picnic': 1475, 'jellyfish': 1476, 'almost': 1477, 'envious': 1478, 'kissed': 1479, 'triangle': 1480, 'twist': 1481, 'necklace': 1482, 'arrow': 1483, 'lulu': 1484, 'sighed': 1485, 'miserable': 1486, 'middle': 1487, 'louder': 1488, 'lizard': 1489, 'hopping': 1490, 'stage': 1491, 'toast': 1492, 'statue': 1493, 'mill': 1494, 'impressive': 1495, 'lead': 1496, 'serious': 1497, 'steal': 1498, 'friendship': 1499, 'careless': 1500, 'pigeon': 1501, 'vegetables': 1502, 'finger': 1503, 'smoke': 1504, 'maze': 1505, 'class': 1506, 'ghost': 1507, 'sailed': 1508, 'coffee': 1509, 'museum': 1510, 'harsh': 1511, 'napkin': 1512, 'belt': 1513, 'bracelet': 1514, 'reliable': 1515, 'feather': 1516, 'slipped': 1517, 'nosy': 1518, 'trick': 1519, 'yet': 1520, 'taken': 1521, 'washed': 1522, 'meant': 1523, 'happiness': 1524, 'hurts': 1525, 'attractive': 1526, 'rat': 1527, 'cries': 1528, 'trumpet': 1529, 'judge': 1530, 'alligator': 1531, 'movie': 1532, 'wealthy': 1533, 'ordinary': 1534, 'mild': 1535, 'spike': 1536, 'filthy': 1537, 'belong': 1538, 'butter': 1539, 'radio': 1540, 'ones': 1541, 'bin': 1542, 'boxes': 1543, 'belonged': 1544, 'guess': 1545, 'forward': 1546, 'walks': 1547, 'gorilla': 1548, 'pan': 1549, 'dolphin': 1550, 'perform': 1551, 'cushion': 1552, 'peach': 1553, 'locked': 1554, 'shore': 1555, 'poured': 1556, 'shade': 1557, 'giving': 1558, 'camera': 1559, 'nod': 1560, 'shells': 1561, 'police': 1562, 'boats': 1563, 'whispered': 1564, 'answered': 1565, 'add': 1566, 'tag': 1567, 'dolly': 1568, 'nods': 1569, 'four': 1570, 'times': 1571, 'regular': 1572, 'drawer': 1573, 'saying': 1574, 'mule': 1575, 'create': 1576, 'buzz': 1577, 'sailor': 1578, 'mole': 1579, 'tomato': 1580, 'shrimp': 1581, 'shoot': 1582, 'lotion': 1583, 'mop': 1584, 'cocoa': 1585, 'cupboard': 1586, 'tank': 1587, 'zipper': 1588, 'octopus': 1589, 'garage': 1590, 'shrink': 1591, 'alert': 1592, 'stronger': 1593, 'chubby': 1594, 'pete': 1595, 'returned': 1596, 'houses': 1597, 'trunk': 1598, 'club': 1599, 'fearful': 1600, 'johnny': 1601, 'successful': 1602, 'lively': 1603, 'those': 1604, 'wool': 1605, 'gum': 1606, 'invited': 1607, 'acorn': 1608, 'crack': 1609, 'nicely': 1610, 'wonder': 1611, 'needle': 1612, 'dough': 1613, 'dreams': 1614, 'showing': 1615, 'witch': 1616, 'desk': 1617, 'bubble': 1618, 'persistent': 1619, 'missile': 1620, 'helps': 1621, 'solve': 1622, '“i': 1623, 'modern': 1624, 'fridge': 1625, 'value': 1626, 'gets': 1627, 'shine': 1628, 'grown': 1629, 'helpless': 1630, 'radish': 1631, 'stretch': 1632, 'present': 1633, 'shout': 1634, 'decide': 1635, 'licked': 1636, 'send': 1637, 'sleeping': 1638, 'cooked': 1639, 'gem': 1640, 'bedroom': 1641, 'loudly': 1642, 'split': 1643, 'candle': 1644, 'bounced': 1645, 'alice': 1646, 'sort': 1647, 'learning': 1648, 'city': 1649, 'truth': 1650, 'bean': 1651, 'wishes': 1652, 'ways': 1653, 'borrow': 1654, 'harder': 1655, 'lit': 1656, 'less': 1657, 'hive': 1658, 'gym': 1659, 'umbrella': 1660, 'jacket': 1661, 'stuff': 1662, 'eraser': 1663, 'automobile': 1664, 'soar': 1665, 'guilty': 1666, 'tunnel': 1667, 'cherries': 1668, 'station': 1669, 'jewelry': 1670, 'pin': 1671, 'cereal': 1672, 'pointing': 1673, 'may': 1674, 'adorable': 1675, 'wheels': 1676, 'sell': 1677, 'jazz': 1678, 'bloom': 1679, 'guard': 1680, 'fountain': 1681, 'thief': 1682, 'cop': 1683, 'biggest': 1684, 'homes': 1685, 'father': 1686, 'ray': 1687, 'stamp': 1688, 'powder': 1689, 'drop': 1690, 'happening': 1691, 'skip': 1692, 'post': 1693, 'repair': 1694, 'library': 1695, 'kim': 1696, 'flashlight': 1697, 'otter': 1698, 'nurse': 1699, 'cactus': 1700, 'office': 1701, 'power': 1702, 'dive': 1703, 'grapes': 1704, 'lying': 1705, 'stir': 1706, 'order': 1707, 'vine': 1708, 'forth': 1709, 'bark': 1710, 'chalk': 1711, 'skipped': 1712, 'sweater': 1713, 'impatient': 1714, 'flute': 1715, 'helmet': 1716, 'hippo': 1717, 'case': 1718, 'lollipop': 1719, 'handle': 1720, 'sink': 1721, 'figure': 1722, 'charming': 1723, 'speak': 1724, 'half': 1725, 'toby': 1726, 'cauliflower': 1727, 'support': 1728, 'counting': 1729, 'thinking': 1730, 'tripped': 1731, 'jet': 1732, 'feathers': 1733, 'spirit': 1734, 'carpet': 1735, 'strawberry': 1736, 'travel': 1737, 'knows': 1738, 'slept': 1739, 'earth': 1740, 'themselves': 1741, 'oak': 1742, 'avocado': 1743, 'staff': 1744, 'wrapped': 1745, 'gifted': 1746, 'yacht': 1747, 'led': 1748, 'spun': 1749, 'cover': 1750, 'shield': 1751, 'competitive': 1752, 'tweet': 1753, 'telephone': 1754, 'relax': 1755, 'elevator': 1756, 'marry': 1757, 'respect': 1758, 'low': 1759, 'thunder': 1760, 'log': 1761, 'skin': 1762, 'quit': 1763, 'dishwasher': 1764, 'magnet': 1765, 'snap': 1766, 'zoomy': 1767, 'towers': 1768, 'sauce': 1769, 'tears': 1770, 'speed': 1771, 'photo': 1772, 'pole': 1773, 'gathered': 1774, 'flea': 1775, 'decorate': 1776, 'tire': 1777, 'giggled': 1778, 'hotel': 1779, 'iron': 1780, 'pirate': 1781, 'toe': 1782, 'slid': 1783, 'ambulance': 1784, 'valuable': 1785, 'buttons': 1786, 'shape': 1787, 'compassionate': 1788, 'anchor': 1789, 'celery': 1790, 'ears': 1791, 'pill': 1792, 'pine': 1793, 'restless': 1794, 'tray': 1795, 'destroy': 1796, 'slides': 1797, 'van': 1798, 'record': 1799, 'numbers': 1800, 'edge': 1801, 'salt': 1802, 'spinning': 1803, 'knot': 1804, 'tear': 1805, 'heal': 1806, 'question': 1807, 'disappointed': 1808, 'shut': 1809, 'scream': 1810, 'meal': 1811, 'stranger': 1812, 'wife': 1813, 'fisherman': 1814, 'intelligent': 1815, 'means': 1816, 'deliver': 1817, 'ouch': 1818, 'bushes': 1819, 'laughs': 1820, 'barrel': 1821, 'return': 1822, 'rubber': 1823, 'bill': 1824, 'bananas': 1825, 'collect': 1826, 'cardboard': 1827, 'patch': 1828, 'oil': 1829, 'birdy': 1830, 'walls': 1831, 'stream': 1832, 'd': 4042, 'either': 1834, 'neighbor': 1835, 'hospital': 1836, 'juicy': 1837, 'soldier': 1838, 'ink': 1839, 'pit': 1840, 'quick': 1841, 'coal': 1842, 'lime': 1843, 'added': 1844, 'cricket': 1845, 'silver': 1846, 'birdcage': 1847, 'distant': 1848, 'safely': 1849, 'crib': 1850, 'spilled': 1851, 'bull': 1852, 'pretending': 1853, 'trap': 1854, 'unlock': 1855, 'modest': 1856, 'dreamed': 1857, 'fork': 1858, 'bikes': 1859, 'spear': 1860, 'tummy': 1861, 'angel': 1862, 'purse': 1863, 'view': 1864, 'weigh': 1865, 'pale': 1866, 'questions': 1867, 'rode': 1868, 'playground': 1869, 'coins': 1870, 'wiped': 1871, 'kindness': 1872, 'costume': 1873, 'shocked': 1874, 'trail': 1875, 'hurry': 1876, 'onion': 1877, 'lights': 1878, 'sophie': 1879, 'aeroplane': 1880, 'yogurt': 1881, 'hook': 1882, 'glove': 1883, 'graceful': 1884, 'pray': 1885, 'itself': 1886, 'palace': 1887, 'flame': 1888, 'falls': 1889, 'mosquito': 1890, 'sandy': 1891, 'sausage': 1892, 'computer': 1893, 'minutes': 1894, 'climbing': 1895, 'rhinoceros': 1896, 'painter': 1897, 'blow': 1898, 'gas': 1899, 'scooter': 1900, 'crystal': 1901, 'knight': 1902, 'tap': 1903, 'refrigerator': 1904, 'golf': 1905, 'twig': 1906, 'swinging': 1907, 'packed': 1908, 'steps': 1909, 'rod': 1910, 'mug': 1911, 'basketball': 1912, 'comet': 1913, 'parade': 1914, 'math': 1915, 'pack': 1916, 'license': 1917, 'mrs': 1918, 'pandy': 1919, 'haircut': 1920, 'lie': 1921, 'leash': 1922, 'hate': 1923, 'newspaper': 1924, 'chain': 1925, 'church': 1926, 'faucet': 1927, 'past': 1928, 'thanks': 1929, 'serve': 1930, 'bandage': 1931, 'driving': 1932, 'disappeared': 1933, 'makeup': 1934, 'picking': 1935, 'television': 1936, 'baseball': 1937, 'knob': 1938, 'brothers': 1939, 'sofa': 1940, 'often': 1941, 'collar': 1942, 'eye': 1943, 'reverse': 1944, 'polish': 1945, 'shampoo': 1946, 'available': 1947, 'video': 1948, 'cable': 1949, 'meadow': 1950, 'peace': 1951, 'determined': 1952, 'week': 1953, 'managed': 1954, 'rubbed': 1955, 'alarm': 1956, 'cherry': 1957, 'penguin': 1958, 'axe': 1959, 'veil': 1960, 'cups': 1961, 'olive': 1962, 'medal': 1963, 'taller': 1964, 'porch': 1965, 'bags': 1966, 'rub': 1967, 'sparkly': 1968, 'metal': 1969, 'cooler': 1970, 'ha': 1971, 'microphone': 1972, 'drawings': 1973, 'ticket': 1974, 'pour': 1975, 'hockey': 1976, 'wire': 1977, 'bathtub': 1978, 'diary': 1979, 'x': 4044, 'rolling': 1981, 'vendor': 1982, 'stared': 1983, 'helicopter': 1984, 'herb': 1985, 'wardrobe': 1986, 'rice': 1987, 'buzzy': 1988, 'gasped': 1989, 'already': 1990, 'organize': 1991, 'gear': 1992, 'bessie': 1993, 'rise': 1994, 'stack': 1995, 'label': 1996, 'model': 1997, 'battery': 1998, 'bookshelf': 1999, 'igloo': 2000, 'stretched': 2001, 'hunter': 2002, 'dare': 2003, 'grape': 2004, 'papers': 2005, 'swung': 2006, 'pastry': 2007, 'pilot': 2008, 'quarrel': 2009, 'ellie': 2010, 'fog': 2011, 'mattress': 2012, 'winter': 2013, 'giraffe': 2014, 'thread': 2015, 'mint': 2016, 'novel': 2017, 'motorcycle': 2018, 'dreaming': 2019, 'fold': 2020, 'jokes': 2021, 'network': 2022, 'cooking': 2023, 'flour': 2024, 'gather': 2025, 'hurricane': 2026, 'realised': 2027, 'design': 2028, 'mustache': 2029, 'tina': 2030, 'blackboard': 2031, 'load': 2032, 'thermometer': 2033, 'fought': 2034, 'wallet': 2035, 'beef': 2036, 'bicycle': 2037, 'notice': 2038, 'aid': 2039, 'falling': 2040, 'buckle': 2041, 'piano': 2042, 'lee': 2043, 'elly': 2044, 'ostrich': 2045, 'crane': 2046, 'bent': 2047, 'calling': 2048, 'sweetie': 2049, 'laser': 2050, 'volcano': 2051, 'trophy': 2052, 'frogs': 2053, 'hours': 2054, 'trucks': 2055, 'sheet': 2056, 'hanging': 2057, 'unpack': 2058, 'sadly': 2059, 'tool': 2060, 'rake': 2061, 'beep': 2062, 'drain': 2063, 'meat': 2064, 'pressed': 2065, 'shelter': 2066, 'yoga': 2067, 'disappear': 2068, 'ten': 2069, 'lumber': 2070, 'trains': 2071, 'apron': 2072, 'bury': 2073, 'reading': 2074, 'hammer': 2075, 'squirrels': 2076, 'pistol': 2077, 'package': 2078, 'against': 2079, 'enjoying': 2080, 'offered': 2081, 'spend': 2082, 'discovered': 2083, 'short': 2084, 'complain': 2085, 'sunset': 2086, 'betty': 2087, 'message': 2088, 'explain': 2089, 'marched': 2090, 'skull': 2091, 'spark': 2092, 'track': 2093, 'blouse': 2094, 'summer': 2095, 'fuel': 2096, 'attach': 2097, 'prunes': 2098, 'hut': 2099, 'ornament': 2100, 'early': 2101, 'stable': 2102, 'nail': 2103, 'factory': 2104, 'zigzag': 2105, 'skirt': 2106, 'curtain': 2107, 'tube': 2108, 'magazine': 2109, 'excitement': 2110, 'practice': 2111, 'task': 2112, 'five': 2113, 'peeked': 2114, 'fingers': 2115, 'stairs': 2116, 'holds': 2117, 'teaspoon': 2118, 'accidentally': 2119, 'boo': 2120, 'sight': 2121, 'parent': 2122, 'son': 2123, 'junk': 2124, 'growing': 2125, 'bathe': 2126, 'journal': 2127, 'paul': 2128, 'disturb': 2129, 'castles': 2130, 'dessert': 2131, 'theater': 2132, 'hanger': 2133, 'matches': 2134, 'football': 2135, 'restore': 2136, 'opens': 2137, 'skeleton': 2138, 'telling': 2139, 'popped': 2140, 'shaking': 2141, 'happier': 2142, 'switch': 2143, 'ear': 2144, 'fastest': 2145, 'allowed': 2146, 'cube': 2147, 'pippin': 2148, 'pear': 2149, 'creature': 2150, 'cord': 2151, 'burn': 2152, 'bathrobe': 2153, 'mitten': 2154, 'kangaroo': 2155, 'cobweb': 2156, 'pipe': 2157, 'zoomed': 2158, 'listening': 2159, 'distance': 2160, 'vroom': 2161, 'ahead': 2162, 'peanut': 2163, 'oxygen': 2164, 'papa': 2165, 'razor': 2166, 'poke': 2167, 'stumbled': 2168, 'enthusiastic': 2169, 'rag': 2170, 'wisdom': 2171, 'dictionary': 2172, 'nation': 2173, 'copper': 2174, 'admire': 2175, 'stirred': 2176, 'alive': 2177, 'surprises': 2178, 'ribbon': 2179, 'insect': 2180, 'brick': 2181, 'stickers': 2182, 'object': 2183, 'helper': 2184, 'scatter': 2185, 'complete': 2186, 'danger': 2187, 'chose': 2188, 'bouncy': 2189, 'trash': 2190, 'violin': 2191, 'jumps': 2192, 'exercise': 2193, 'dust': 2194, 'chasing': 2195, 'symbol': 2196, 'laundry': 2197, 'circles': 2198, 'stops': 2199, 'respectful': 2200, 'point': 2201, 'mineral': 2202, 'monkeys': 2203, 'breakfast': 2204, 'neat': 2205, 'nature': 2206, 'weapon': 2207, 'screw': 2208, 'lions': 2209, 'accept': 2210, 'war': 2211, 'freezer': 2212, 'waffle': 2213, 'blinked': 2214, 'refused': 2215, 'lightning': 2216, 'festival': 2217, ';': 4037, 'leaving': 2219, 'navy': 2220, 'toilet': 2221, 'player': 2222, 'language': 2223, 'poem': 2224, 'nobody': 2225, 'doors': 2226, 'fireplace': 2227, 'stadium': 2228, 'berry': 2229, 'oats': 2230, 'repeat': 2231, 'cell': 2232, 'spoke': 2233, 'imagine': 2234, 'pleased': 2235, 'searching': 2236, 'lilly': 2237, 'meowed': 2238, 'lemons': 2239, 'sniffed': 2240, 'rules': 2241, 'airport': 2242, 'cabinet': 2243, 'hang': 2244, 'tooth': 2245, 'sneeze': 2246, 'traffic': 2247, 'forgave': 2248, 'tightly': 2249, 'slap': 2250, 'cows': 2251, 'whip': 2252, 'freeze': 2253, 'sandwiches': 2254, 'lawn': 2255, 'doesn': 2256, 'ashtray': 2257, 'leak': 2258, 'peek': 2259, 'mittens': 2260, 'caring': 2261, 'finds': 2262, 'anyway': 2263, 'favourite': 2264, 'roared': 2265, 'cheer': 2266, 'unite': 2267, 'meeting': 2268, 'separate': 2269, 'tomatoes': 2270, 'sticker': 2271, 'pony': 2272, 'tuna': 2273, 'mechanic': 2274, 'boots': 2275, 'hall': 2276, 'plastic': 2277, 'exam': 2278, 'blueberry': 2279, 'check': 2280, 'beetle': 2281, 'wiggly': 2282, 'grill': 2283, 'polly': 2284, 'itch': 2285, 'barking': 2286, 'grabs': 2287, 'pattern': 2288, 'human': 2289, 'hedge': 2290, 'camp': 2291, 'mixer': 2292, 'poked': 2293, 'billboard': 2294, 'letters': 2295, 'succeed': 2296, 'minute': 2297, 'reward': 2298, 'lend': 2299, 'subway': 2300, 'holes': 2301, 'display': 2302, 'wine': 2303, 'pinch': 2304, 'belongs': 2305, 'spell': 2306, 'chimney': 2307, 'cards': 2308, 'mistakes': 2309, 'above': 2310, 'handed': 2311, 'size': 2312, 'resist': 2313, 'remove': 2314, 'wander': 2315, 'invite': 2316, 'shiver': 2317, 'pair': 2318, 'goal': 2319, 'notebook': 2320, 'holiday': 2321, 'film': 2322, 'laughter': 2323, 'classroom': 2324, 'oyster': 2325, 'leaned': 2326, 'puddles': 2327, 'woof': 2328, 'prevent': 2329, 'musician': 2330, 'sent': 2331, 'names': 2332, 'improve': 2333, 'chamber': 2334, 'during': 2335, 'proudly': 2336, 'sudden': 2337, 'rescue': 2338, 'surrender': 2339, 'bookcase': 2340, 'glowing': 2341, 'wipe': 2342, 'problems': 2343, 'warned': 2344, 'swamp': 2345, 'gigi': 2346, 'spoil': 2347, 'strawberries': 2348, '“let’s': 2349, 'windows': 2350, 'loop': 2351, 'slippery': 2352, 'agree': 2353, 'dancer': 2354, 'butterflies': 2355, 'pushes': 2356, 'vest': 2357, 'veterinarian': 2358, 'pulls': 2359, 'ceiling': 2360, 'cliff': 2361, 'keeping': 2362, 'pages': 2363, 'sandbox': 2364, 'shovel': 2365, 'soil': 2366, 'armchair': 2367, 'frowned': 2368, 'excitedly': 2369, 'courage': 2370, 'mailman': 2371, 'panic': 2372, 'trade': 2373, 'brightly': 2374, 'planes': 2375, 'broccoli': 2376, 'carrying': 2377, 'freddy': 2378, 'dear': 2379, 'tease': 2380, 'page': 2381, 'squeeze': 2382, 'kinds': 2383, 'remain': 2384, 'ignore': 2385, 'heat': 2386, 'feelings': 2387, 'passport': 2388, 'rot': 2389, 'stomach': 2390, 'provide': 2391, 'success': 2392, 'rail': 2393, 'energy': 2394, 'slip': 2395, 'lab': 2396, 'comfort': 2397, 'believed': 2398, 'strip': 2399, 'susie': 2400, 'kayak': 2401, 'god': 2402, 'somewhere': 2403, 'prayed': 2404, 'seem': 2405, 'shakes': 2406, 'tools': 2407, 'melt': 2408, 'microscope': 2409, 'calendar': 2410, 'raise': 2411, 'waste': 2412, 'binky': 2413, 'mike': 2414, 'older': 2415, 'lid': 2416, 'saving': 2417, 'stair': 2418, 'reminded': 2419, 'lives': 2420, 'fear': 2421, 'yuck': 2422, 'mystery': 2423, 'sounded': 2424, 'advice': 2425, 'wednesday': 2426, 'lawyer': 2427, 'sandcastle': 2428, 'fort': 2429, 'letting': 2430, 'flies': 2431, 'vehicle': 2432, 'anywhere': 2433, 'chairs': 2434, 'flood': 2435, 'cane': 2436, 'suggested': 2437, 'shone': 2438, 'shouts': 2439, 'colourful': 2440, 'tip': 2441, 'crossed': 2442, 'rushed': 2443, 'imagined': 2444, 'eaten': 2445, 'yell': 2446, 'softly': 2447, '“yes': 2448, 'chickens': 2449, 'starts': 2450, 'law': 2451, 'rooms': 2452, 'breaks': 2453, 'happens': 2454, 'grace': 2455, 'knock': 2456, 'maggie': 2457, 'zebra': 2458, 'poison': 2459, 'stitch': 2460, 'smells': 2461, 'replace': 2462, 'restaurant': 2463, 'pitch': 2464, 'taxi': 2465, 'planet': 2466, 'prune': 2467, 'hung': 2468, 'price': 2469, 'steam': 2470, 'riding': 2471, 'blowing': 2472, 'ranch': 2473, 'rained': 2474, 'file': 2475, 'reindeer': 2476, 'milly': 2477, 'lola': 2478, 'george': 2479, 'ended': 2480, 'it’s': 2481, 'delighted': 2482, 'gain': 2483, 'frown': 2484, 'dishes': 2485, 'superhero': 2486, 'blueberries': 2487, 'chewed': 2488, 'bouncing': 2489, 'barks': 2490, 'spade': 2491, 'sneezed': 2492, 'charlie': 2493, 'alex': 2494, 'tutor': 2495, 'prepare': 2496, 'don’t': 2497, 'settle': 2498, 'heads': 2499, 'hoping': 2500, 'argue': 2501, 'velvet': 2502, 'myself': 2503, 'tornado': 2504, 'carl': 2505, 'hoppy': 2506, 'quiz': 2507, 'delight': 2508, 'art': 2509, 'chance': 2510, 'baking': 2511, 'shrugged': 2512, 'yawn': 2513, 'writing': 2514, 'opera': 2515, 'millie': 2516, 'fresh': 2517, 'giggle': 2518, 'burned': 2519, 'captain': 2520, 'chores': 2521, 'missing': 2522, 'pat': 2523, 'evening': 2524, 'squeaky': 2525, 'shivering': 2526, 'firework': 2527, 'james': 2528, 'prints': 2529, 'weather': 2530, 'perfume': 2531, 'equipment': 2532, 'snowy': 2533, 'vacation': 2534, 'lizzy': 2535, 'fixing': 2536, 'yay': 2537, 'float': 2538, 'extra': 2539, 'yield': 2540, 'sir': 2541, 'country': 2542, 'ruby': 2543, 'zero': 2544, 'valley': 2545, 'event': 2546, 'pushing': 2547, 'sweetheart': 2548, 'placed': 2549, 'below': 2550, 'bumped': 2551, 'afternoon': 2552, 'spots': 2553, 'throws': 2554, 'trusted': 2555, 'throwing': 2556, 'hills': 2557, 'yawned': 2558, 'puzzles': 2559, 'socks': 2560, 'speedy': 2561, 'insisted': 2562, 'tracks': 2563, 'parts': 2564, 'surf': 2565, 'university': 2566, 'froggy': 2567, 'coats': 2568, 'beans': 2569, 'candles': 2570, 'root': 2571, 'mall': 2572, 'brighter': 2573, 'sticky': 2574, 'cared': 2575, 'greedy': 2576, 'following': 2577, 'shoulder': 2578, 'structure': 2579, 'shower': 2580, 'printed': 2581, 'impressed': 2582, 'oasis': 2583, 'unhappy': 2584, 'teaching': 2585, 'panda': 2586, 'claws': 2587, 'expect': 2588, 'racing': 2589, 'deeper': 2590, 'luggage': 2591, 'steel': 2592, 'discover': 2593, 'delay': 2594, 'clapping': 2595, 'content': 2596, 'mist': 2597, 'yucky': 2598, 'easily': 2599, 'enter': 2600, 'sprayed': 2601, 'splashing': 2602, 'score': 2603, 'sparkled': 2604, 'sport': 2605, 'henry': 2606, 'mandy': 2607, 'tilly': 2608, 'checked': 2609, 'learnt': 2610, 'tub': 2611, 'ignored': 2612, 'regret': 2613, 'follows': 2614, 'destroyed': 2615, 'lemonade': 2616, 'coco': 2617, 'scattered': 2618, 'steve': 2619, 'selling': 2620, 'tremble': 2621, 'cutting': 2622, 'furniture': 2623, 'upstairs': 2624, 'vanished': 2625, 'worms': 2626, 'buried': 2627, 'reef': 2628, 'marriage': 2629, 'cabin': 2630, 'kate': 2631, 'entered': 2632, 'jogged': 2633, 'badly': 2634, 'between': 2635, 'whistled': 2636, 'jail': 2637, 'rush': 2638, 'weep': 2639, 'kiki': 2640, 'permit': 2641, 'sorting': 2642, 'weight': 2643, 'finn': 2644, 'creek': 2645, 'counter': 2646, 'exclaimed': 2647, 'marble': 2648, 'style': 2649, 'points': 2650, 'hopes': 2651, 'abby': 2652, 'bang': 2653, 'massage': 2654, 'paid': 2655, 'didn’t': 2656, 'bears': 2657, 'hen': 2658, 'quite': 2659, 'danny': 2660, 'bricks': 2661, 'seats': 2662, 'ash': 2663, 'hurting': 2664, 'climbs': 2665, 'breathe': 2666, 'petals': 2667, 'supply': 2668, 'tug': 2669, 'shopkeeper': 2670, 'raining': 2671, 'putting': 2672, 'grant': 2673, 'warn': 2674, 'boss': 2675, 'cakes': 2676, 'invitation': 2677, 'forgotten': 2678, 'press': 2679, 'clara': 2680, 'strength': 2681, 'nana': 2682, 'menu': 2683, 'jogging': 2684, 'starting': 2685, 'wags': 2686, 'ingredients': 2687, 'explored': 2688, 'admired': 2689, 'zara': 2690, 'introduce': 2691, 'port': 2692, 'sold': 2693, 'washing': 2694, 'tour': 2695, 'pip': 2696, '–': 4049, 'works': 2698, 'waving': 2699, 'finny': 2700, 'pigs': 2701, 'printer': 2702, 'bop': 2703, 'drops': 2704, 'stealing': 2705, 'fallen': 2706, 'meow': 2707, 'apart': 2708, 'plates': 2709, 'sailing': 2710, 'further': 2711, 'blood': 2712, 'quack': 2713, 'eagerly': 2714, 'shined': 2715, 'whisper': 2716, 'wreck': 2717, 'suzy': 2718, 'folded': 2719, 'luck': 2720, 'observe': 2721, 'thumb': 2722, 'choice': 2723, 'warning': 2724, 'especially': 2725, 'acted': 2726, 'floated': 2727, 'couldn’t': 2728, 'increase': 2729, 'soccer': 2730, 'blink': 2731, 'crash': 2732, 'moves': 2733, 'wandered': 2734, 'second': 2735, 'beauty': 2736, 'horses': 2737, 'dressed': 2738, 'torn': 2739, 'uniform': 2740, 'studied': 2741, 'dave': 2742, 'planted': 2743, 'bones': 2744, 'plays': 2745, 'highest': 2746, 'alright': 2747, 'flapped': 2748, 'operation': 2749, 'worth': 2750, 'heel': 2751, 'examine': 2752, 'kingdom': 2753, 'snail': 2754, 'elephants': 2755, 'since': 2756, 'urge': 2757, 'mister': 2758, 'discuss': 2759, 'toto': 2760, 'shouting': 2761, 'accident': 2762, 'given': 2763, 'harry': 2764, 'reaches': 2765, 'punish': 2766, 'watered': 2767, 'presents': 2768, 'soak': 2769, 'strike': 2770, 'brings': 2771, 'isn': 2772, 'solved': 2773, 'immediately': 2774, 'lecture': 2775, 'harbor': 2776, 'west': 2777, 'twisted': 2778, 'snapped': 2779, 'scolded': 2780, 'slapped': 2781, 'gifts': 2782, 'quietly': 2783, 'sorts': 2784, 'sparkle': 2785, 'earthquake': 2786, 'hearts': 2787, 'tongue': 2788, 'oranges': 2789, 'sits': 2790, 'seeing': 2791, 'curled': 2792, 'ava': 2793, 'motor': 2794, 'lands': 2795, 'sliding': 2796, 'let’s': 2797, 'someday': 2798, 'wink': 2799, 'tuesday': 2800, 'yesterday': 2801, 'yum': 2802, 'kisses': 2803, 'planned': 2804, 'memory': 2805, 'image': 2806, 'bend': 2807, 'attached': 2808, 'worse': 2809, 'recommend': 2810, 'paused': 2811, 'fields': 2812, 'stuffed': 2813, '“what': 2814, 'tide': 2815, 'sizes': 2816, 'licks': 2817, 'squeezed': 2818, 'skills': 2819, 'hadn': 2820, 'pulling': 2821, 'ruler': 2822, 'wally': 2823, 'whistling': 2824, 'ideas': 2825, 'sorted': 2826, 'accepted': 2827, 'scratch': 2828, 'catching': 2829, 'manage': 2830, 've': 2831, 'release': 2832, 'dan': 2833, 'cape': 2834, 'colours': 2835, 'pearl': 2836, 'robby': 2837, 'katie': 2838, 'peaches': 2839, 'dried': 2840, 'sunrise': 2841, 'tonight': 2842, 'zookeeper': 2843, 'mice': 2844, 'lip': 2845, 'measured': 2846, 'news': 2847, 'allow': 2848, 'welcomed': 2849, 'rabbits': 2850, 'attention': 2851, 'applaud': 2852, 'leather': 2853, 'twirled': 2854, 'blankets': 2855, 'everyday': 2856, 'ours': 2857, 'celebrated': 2858, 'calls': 2859, 'tucked': 2860, 'shot': 2861, 'settled': 2862, 'exit': 2863, 'downstairs': 2864, 'season': 2865, 'toes': 2866, 'melted': 2867, 'growled': 2868, 'wagging': 2869, 'embarrass': 2870, '“it’s': 2871, 'closely': 2872, 'peanuts': 2873, 'difference': 2874, 'joey': 2875, 'recognize': 2876, 'baked': 2877, 'disagreed': 2878, 'winning': 2879, 'dresses': 2880, 'grocery': 2881, 'remained': 2882, 'imagination': 2883, '“that': 2884, 'fireman': 2885, 'married': 2886, 'tastes': 2887, 'memories': 2888, 'safari': 2889, 'ups': 2890, 'annie': 2891, 'understanding': 2892, 'soaked': 2893, 'receive': 2894, 'appear': 2895, 'zipped': 2896, 'wooden': 2897, 'stirring': 2898, 'experience': 2899, 'snuggled': 2900, 'however': 2901, 'vegetable': 2902, 'punished': 2903, 'lips': 2904, 'acorns': 2905, 'maria': 2906, 'tore': 2907, 'potatoes': 2908, 'encouraged': 2909, 'gloves': 2910, 'pop': 2911, 'shed': 2912, 'unfortunately': 2913, 'remind': 2914, 'easier': 2915, 'breaking': 2916, 'universe': 2917, 'include': 2918, 'rhythm': 2919, 'cheek': 2920, 'girls': 2921, 'rang': 2922, 'collection': 2923, 'creatures': 2924, 'hits': 2925, 'rob': 2926, 'kicking': 2927, 'trembling': 2928, '“no': 2929, 'straight': 2930, 'pirates': 2931, 'floating': 2932, 'crawling': 2933, 'gazed': 2934, 'raised': 2935, 'sidewalk': 2936, 'pebbles': 2937, 'flap': 2938, 'ringing': 2939, 'ted': 2940, 'tony': 2941, 'relaxed': 2942, 'babies': 2943, 'shopping': 2944, 'waits': 2945, 'hers': 2946, 'flat': 2947, 'fireworks': 2948, 'unlocked': 2949, 'kites': 2950, 'bites': 2951, 'smith': 2952, 'muscles': 2953, 'cuddles': 2954, 'palm': 2955, 'dared': 2956, 'pastel': 2957, 'drinking': 2958, 'families': 2959, 'watches': 2960, 'cindy': 2961, 'liz': 2962, 'pinky': 2963, 'josh': 2964, 'cartoon': 2965, 'rosie': 2966, 'kneeled': 2967, 'lets': 2968, 'studying': 2969, 'shouldn': 2970, 'none': 2971, 'digging': 2972, 'resting': 2973, 'silence': 2974, 'petted': 2975, 'breeze': 2976, 'shadows': 2977, 'keeps': 2978, 'visited': 2979, 'safety': 2980, 'mouths': 2981, 'tallest': 2982, 'matched': 2983, 'chewing': 2984, 'underground': 2985, 'muddy': 2986, 'treasures': 2987, 'playtime': 2988, 'ordered': 2989, 'vet': 2990, 'weeping': 2991, 'beds': 2992, 'laid': 2993, 'amazement': 2994, 'napping': 2995, 'lined': 2996, 'served': 2997, 'vanish': 2998, 'suffer': 2999, 'brain': 3000, 'sausages': 3001, 'weighed': 3002, 'sigh': 3003, 'piles': 3004, 'cartoons': 3005, 'pup': 3006, 'eagle': 3007, 'dusty': 3008, 'roads': 3009, 'six': 3010, 'winner': 3011, 'encourage': 3012, 'setting': 3013, 'boom': 3014, 'costumes': 3015, 'nowhere': 3016, 'rocky': 3017, 'david': 3018, 'roots': 3019, 'mixing': 3020, 'signs': 3021, 'spoons': 3022, 'landscape': 3023, 'sunday': 3024, 'happiest': 3025, 'importance': 3026, 'sisters': 3027, 'decorated': 3028, 'urged': 3029, 'row': 3030, 'pets': 3031, 'lean': 3032, 'fetch': 3033, 'repeated': 3034, 'pinched': 3035, 'voices': 3036, 'perfectly': 3037, 'decorations': 3038, 'gust': 3039, 'history': 3040, 'sip': 3041, 'crunchy': 3042, 'patterns': 3043, 'guessed': 3044, 'mentioned': 3045, 'knights': 3046, 'strangers': 3047, 'unicorn': 3048, 'prepared': 3049, 'infant': 3050, 'draws': 3051, 'noon': 3052, 'agreement': 3053, 'result': 3054, 'behaved': 3055, 'zoe': 3056, 'known': 3057, 'clearing': 3058, 'poems': 3059, 'waffles': 3060, 'beard': 3061, 'escaped': 3062, 'bother': 3063, 'moments': 3064, 'matching': 3065, 'turning': 3066, 'reveal': 3067, 'fashion': 3068, 'ourselves': 3069, 'wag': 3070, 'marks': 3071, 'lung': 3072, 'limes': 3073, 'grinned': 3074, 'spending': 3075, 'web': 3076, 'ripped': 3077, 'bowed': 3078, 'emergency': 3079, 'cuddled': 3080, 'bravely': 3081, 'crabs': 3082, 'fishing': 3083, 'apologized': 3084, 'logs': 3085, 'shh': 3086, 'goals': 3087, 'crashed': 3088, 'offer': 3089, 'disturbed': 3090, 'pillows': 3091, 'frozen': 3092, 'soldiers': 3093, 'catty': 3094, 'beneath': 3095, 'disagree': 3096, 'gained': 3097, 'collecting': 3098, 'jacob': 3099, 'expected': 3100, 'photos': 3101, 'lines': 3102, 'knelt': 3103, 'skipping': 3104, 'struggle': 3105, 'dads': 3106, 'hurried': 3107, '“that’s': 3108, 'east': 3109, 'izzy': 3110, 'keeper': 3111, 'bridges': 3112, 'stretching': 3113, 'burning': 3114, 'answers': 3115, '“it': 3116, 'rested': 3117, 'moo': 3118, 'wondering': 3119, 'drinks': 3120, 'sore': 3121, 'trapped': 3122, 'pies': 3123, 'sneak': 3124, '“thank': 3125, 'begged': 3126, 'collected': 3127, 'warmth': 3128, 'dragons': 3129, 'frank': 3130, 'soared': 3131, 'challenge': 3132, 'henny': 3133, 'buildings': 3134, 'batter': 3135, 'praised': 3136, 'joking': 3137, 'flash': 3138, 'kindly': 3139, 'opinion': 3140, 'scratched': 3141, 'wonders': 3142, 'approached': 3143, 'wiggle': 3144, 'ago': 3145, 'beat': 3146, 'stamps': 3147, 'ropes': 3148, 'lied': 3149, 'thomas': 3150, 'teasing': 3151, '“you': 3152, 'scaring': 3153, 'screams': 3154, 'jessie': 3155, 'lena': 3156, 'luckily': 3157, 'sparky': 3158, 'bree': 3159, 'marie': 3160, 'twigs': 3161, 'comfy': 3162, 'sew': 3163, 'scrubbed': 3164, 'neighborhood': 3165, 'texture': 3166, 'gathering': 3167, 'brushed': 3168, 'windy': 3169, '“why': 3170, 'sparkling': 3171, 'golden': 3172, 'theirs': 3173, 'request': 3174, 'olives': 3175, 'broom': 3176, 'ladybug': 3177, 'fed': 3178, 'copy': 3179, 'muffins': 3180, 'pedal': 3181, 'grey': 3182, 'bat': 3183, 'strings': 3184, 'gina': 3185, 'beamed': 3186, 'cubes': 3187, 'deal': 3188, 'shoulders': 3189, 'teaches': 3190, 'pedals': 3191, '“don’t': 3192, 'granted': 3193, 'rover': 3194, 'snakes': 3195, 'gaze': 3196, 'herbs': 3197, 'remembers': 3198, 'chicky': 3199, 'titi': 3200, 'created': 3201, 'investigate': 3202, 'slimy': 3203, 'alien': 3204, 'awe': 3205, 'dino': 3206, 'glowed': 3207, 'argued': 3208, 'recorded': 3209, 'hissed': 3210, 'writes': 3211, 'melting': 3212, 'brushes': 3213, 'received': 3214, 'loose': 3215, 'goodnight': 3216, 'eats': 3217, 'waist': 3218, 'trips': 3219, 'sneaked': 3220, '“i’m': 3221, 'mo': 3222, 'dodo': 3223, 'hardly': 3224, 'thrilled': 3225, 'roaring': 3226, 'shock': 3227, 'balancing': 3228, 'pencils': 3229, 'headed': 3230, 'officer': 3231, 'zooming': 3232, 'parties': 3233, 'simple': 3234, 'marching': 3235, 'avocados': 3236, 'leader': 3237, 'twin': 3238, 'gogo': 3239, 'sweeties': 3240, 'mmm': 3241, 'spit': 3242, 'nicer': 3243, '“we': 3244, 'luxury': 3245, 'sticking': 3246, 'removed': 3247, '“look': 3248, 'supposed': 3249, 'wears': 3250, 'healed': 3251, 'destination': 3252, 'cheering': 3253, 'faraway': 3254, 'slice': 3255, 'screaming': 3256, 'students': 3257, 'mention': 3258, 'decides': 3259, 'yelling': 3260, 'pots': 3261, 'jewels': 3262, 'tugged': 3263, 'mountains': 3264, 'area': 3265, 'traveled': 3266, 'praying': 3267, 'chirped': 3268, 'snowball': 3269, 'luke': 3270, 'excite': 3271, 'julie': 3272, 'inch': 3273, 'remote': 3274, 'radishes': 3275, 'bo': 3276, 'marbles': 3277, 'realize': 3278, 'bump': 3279, 'revealed': 3280, 'threat': 3281, 'stripes': 3282, 'owned': 3283, 'afterwards': 3284, 'hated': 3285, 'claps': 3286, 'i’m': 3287, 'delivered': 3288, 'buckets': 3289, 'polished': 3290, 'barry': 3291, 'wherever': 3292, 'monsters': 3293, 'men': 3294, 'robert': 3295, 'tricked': 3296, 'hitting': 3297, 'backpack': 3298, 'youth': 3299, 'faced': 3300, 'twinkle': 3301, 'kneel': 3302, 'beside': 3303, 'touching': 3304, 'nick': 3305, 'chloe': 3306, 'parked': 3307, 'feeding': 3308, 'tickle': 3309, 'cuts': 3310, 'measuring': 3311, 'acting': 3312, 'bakery': 3313, 'whee': 3314, 'sneaky': 3315, 'continue': 3316, 'magnets': 3317, 'earlier': 3318, 'giraffes': 3319, 'complaining': 3320, 'achoo': 3321, 'cough': 3322, 'oat': 3323, 'mane': 3324, 'seems': 3325, 'staying': 3326, 'opening': 3327, '“hello': 3328, 'provided': 3329, 'safer': 3330, 'secrets': 3331, 'permission': 3332, 'crew': 3333, 'ally': 3334, 'discovery': 3335, 'sting': 3336, 'shooting': 3337, 'seven': 3338, 'heroes': 3339, 'darker': 3340, 'actually': 3341, 'loving': 3342, 'bleeding': 3343, 'dinosaurs': 3344, 'twisty': 3345, 'candies': 3346, 'fears': 3347, 'paints': 3348, 'awesome': 3349, 'actions': 3350, 'lenny': 3351, 'refuse': 3352, 'fifi': 3353, 'tails': 3354, 'struggled': 3355, 'pupil': 3356, 'horns': 3357, 'pans': 3358, 'hippy': 3359, 'pippa': 3360, 'rule': 3361, 'lina': 3362, 'thud': 3363, 'liquid': 3364, 'startled': 3365, 'timer': 3366, 'teamwork': 3367, 'wedding': 3368, 'whatever': 3369, 'creation': 3370, 'weeks': 3371, 'neighbors': 3372, 'helmets': 3373, 'foods': 3374, 'died': 3375, 'patiently': 3376, 'struggling': 3377, 'onwards': 3378, 'tara': 3379, 'hopper': 3380, 'prettiest': 3381, 'anytime': 3382, 'neighbourhood': 3383, 'company': 3384, 'surrounded': 3385, 'awake': 3386, 'flags': 3387, 'spaceship': 3388, 'steep': 3389, 'nightmare': 3390, 'bedtime': 3391, 'glasses': 3392, 'restored': 3393, 'wasted': 3394, 'can’t': 3395, 'valued': 3396, 'nails': 3397, 'spraying': 3398, 'jessica': 3399, 'tune': 3400, 'wanting': 3401, 'cub': 3402, 'santa': 3403, 'passports': 3404, 'patience': 3405, 'pride': 3406, 'peppers': 3407, 'crept': 3408, 'recognized': 3409, 'brian': 3410, 'catches': 3411, 'click': 3412, 'polishing': 3413, 'sights': 3414, 'sings': 3415, 'quacked': 3416, 'paintings': 3417, 'appreciate': 3418, 'admiring': 3419, 'bloomed': 3420, 'succeeded': 3421, 'loaded': 3422, 'jamie': 3423, 'contest': 3424, 'paying': 3425, 'poking': 3426, 'chick': 3427, 'pause': 3428, 'scooters': 3429, 'worker': 3430, 'starfish': 3431, 'frosting': 3432, 'meg': 3433, 'jilly': 3434, 'al': 3435, 'penguins': 3436, 'jenna': 3437, 'foxy': 3438, 'sofia': 3439, 'organizing': 3440, 'rescued': 3441, 'celebration': 3442, 'demand': 3443, 'pears': 3444, '“this': 3445, 'eight': 3446, 'scattering': 3447, 'ribbons': 3448, 'sparkles': 3449, 'sadness': 3450, 'laying': 3451, 'visiting': 3452, 'uh': 3453, 'mission': 3454, 'sighs': 3455, 'cones': 3456, 'instructions': 3457, 'splashes': 3458, 'traded': 3459, 'birdie': 3460, 'balanced': 3461, 'cushions': 3462, 'bringing': 3463, 'blows': 3464, 'blooming': 3465, 'diving': 3466, 'whose': 3467, 'bumpy': 3468, 'guys': 3469, 'scooped': 3470, 'bet': 3471, 'rollie': 3472, 'beads': 3473, 'airplane': 3474, 'conflict': 3475, 'apologize': 3476, 'reluctantly': 3477, 'spill': 3478, 'cause': 3479, 'froze': 3480, 'demanded': 3481, 'protected': 3482, 'applauded': 3483, 'baker': 3484, 'usual': 3485, 'wizard': 3486, 'protecting': 3487, 'purred': 3488, 'learns': 3489, 'scales': 3490, 'jess': 3491, 'explorers': 3492, 'backward': 3493, 'betsy': 3494, 'grandmother': 3495, 'drives': 3496, 'project': 3497, 'reminder': 3498, 'rockets': 3499, 'examined': 3500, 'homework': 3501, 'buggy': 3502, 'colour': 3503, 'untie': 3504, 'honk': 3505, 'brownie': 3506, 'spells': 3507, 'dara': 3508, 'glitter': 3509, 'smiley': 3510, 'scoop': 3511, 'recording': 3512, 'planets': 3513, 'spiders': 3514, 'released': 3515, 'prettier': 3516, 'victory': 3517, 'dragged': 3518, 'forgiving': 3519, 'spikes': 3520, 'cracked': 3521, 'anne': 3522, 'smarter': 3523, 'changing': 3524, 'exactly': 3525, 'barely': 3526, 'lick': 3527, 'dipped': 3528, 'wins': 3529, 'struck': 3530, 'becoming': 3531, 'muscle': 3532, 'stare': 3533, 'shivered': 3534, 'leading': 3535, 'toddler': 3536, 'smallest': 3537, 'helpers': 3538, 'neighbour': 3539, 'figures': 3540, 'suzie': 3541, 'fishy': 3542, 'tractor': 3543, 'rides': 3544, 'guns': 3545, 'abigail': 3546, 'lou': 3547, 'robo': 3548, 'pastels': 3549, 'packing': 3550, 'bowls': 3551, 'stole': 3552, 'that’s': 3553, 'neither': 3554, 'warnings': 3555, 'list': 3556, 'delivering': 3557, 'rainbows': 3558, 'hugging': 3559, 'united': 3560, 'inviting': 3561, 'sprinkles': 3562, 'precious': 3563, '“what’s': 3564, 'buzzing': 3565, 'die': 3566, 'ruin': 3567, 'sunlight': 3568, 'shirts': 3569, 'firemen': 3570, 'rings': 3571, 'ew': 3572, 'bar': 3573, 'holly': 3574, 'covers': 3575, '“oh': 3576, 'sled': 3577, 'mel': 3578, 'activities': 3579, 'princesses': 3580, 'fault': 3581, 'yeah': 3582, 'arguing': 3583, 'sense': 3584, 'misses': 3585, 'complained': 3586, 'tick': 3587, '“can': 3588, 'somebody': 3589, 'wraps': 3590, 'peas': 3591, 'growl': 3592, 'villagers': 3593, 'quarreling': 3594, 'sending': 3595, 'everybody': 3596, 'shrug': 3597, 'arrive': 3598, 'pajamas': 3599, 'vines': 3600, 'snowballs': 3601, 'oops': 3602, 'joked': 3603, 'sports': 3604, 'masks': 3605, 'naps': 3606, 'possible': 3607, 'gotten': 3608, 'peered': 3609, '’': 4033, 'curiosity': 3611, 'cuddle': 3612, 'pins': 3613, 'wiggled': 3614, 'situation': 3615, 'tickets': 3616, 'piggy': 3617, 'bank': 3618, 'separated': 3619, 'chips': 3620, 'rid': 3621, 'cocoon': 3622, 'sadie': 3623, 'toot': 3624, 'spins': 3625, 'kicks': 3626, 'knocks': 3627, 'chirping': 3628, 'cuddly': 3629, 'purr': 3630, 'brand': 3631, 'completed': 3632, 'caused': 3633, 'choices': 3634, 'nine': 3635, 'rocking': 3636, 'belly': 3637, 'improved': 3638, 'excuse': 3639, 'increased': 3640, 'reply': 3641, 'fascinated': 3642, 'biting': 3643, 'blocking': 3644, 'itchy': 3645, 'yells': 3646, 'tricky': 3647, 'usually': 3648, 'motion': 3649, 'sides': 3650, 'scarves': 3651, 'teased': 3652, 'farther': 3653, 'perched': 3654, 'growling': 3655, 'powers': 3656, 'discovering': 3657, 'forehead': 3658, 'plenty': 3659, 'superheroes': 3660, 'pockets': 3661, 'soaring': 3662, 'snatched': 3663, 'paths': 3664, 'requested': 3665, 'properly': 3666, 'groups': 3667, 'widened': 3668, 'adam': 3669, 'behaving': 3670, 'i’ll': 3671, 'creams': 3672, 'items': 3673, 'truly': 3674, 'mailed': 3675, 'janie': 3676, 'twirl': 3677, 'bounces': 3678, 'dr': 3679, 'hoot': 3680, 'bells': 3681, 'stella': 3682, 'sniff': 3683, 'horsy': 3684, 'belle': 3685, 'rumble': 3686, 'worries': 3687, 'giggling': 3688, 'shrinking': 3689, 'buzzed': 3690, 'creating': 3691, 'observed': 3692, 'gross': 3693, 'weren': 3694, 'reaching': 3695, 'contain': 3696, 'although': 3697, 'pointy': 3698, 'dot': 3699, 'folding': 3700, 'honked': 3701, 'swims': 3702, 'chases': 3703, 'super': 3704, 'roses': 3705, 'nests': 3706, 'promises': 3707, 'curtains': 3708, 'sobbed': 3709, 'hallway': 3710, 'stands': 3711, 'throat': 3712, 'rewarded': 3713, 'future': 3714, 'jojo': 3715, 'policeman': 3716, 'chop': 3717, 'dived': 3718, 'baa': 3719, 'tunnels': 3720, 'direction': 3721, 'seagull': 3722, 'listens': 3723, 'polar': 3724, 'zee': 3725, 'jo': 3726, 'images': 3727, 'siblings': 3728, 'stays': 3729, 'sank': 3730, 'celebrating': 3731, 'scurried': 3732, 'completely': 3733, 'corners': 3734, 'staring': 3735, 'agrees': 3736, 'pretends': 3737, 'peeking': 3738, 'fabric': 3739, 'leaning': 3740, 'vanilla': 3741, 'fido': 3742, 'lettuce': 3743, 'napkins': 3744, 'suffered': 3745, 'seesaw': 3746, 'humans': 3747, 'effort': 3748, 'handful': 3749, 'bodies': 3750, 'noses': 3751, 'dropping': 3752, 'beyond': 3753, 'growls': 3754, 'jacky': 3755, 'whispers': 3756, 'basement': 3757, 'tasks': 3758, 'deserve': 3759, 'dolphins': 3760, 'movies': 3761, 'olly': 3762, 'merry': 3763, 'untied': 3764, 'biggie': 3765, 'cans': 3766, 'bracelets': 3767, 'isabella': 3768, 'sponge': 3769, 'interested': 3770, 'siren': 3771, 'grows': 3772, 'firefly': 3773, 'tock': 3774, 'potion': 3775, 'kaylee': 3776, 'sweets': 3777, 'overjoyed': 3778, 'wobbly': 3779, 'footsteps': 3780, 'nutty': 3781, 'reads': 3782, 'repaired': 3783, 'squishy': 3784, 'dots': 3785, 'supplies': 3786, 'backwards': 3787, 'replaced': 3788, 'tickled': 3789, 'wishing': 3790, 'suggest': 3791, 'obey': 3792, 'ventured': 3793, 'keys': 3794, 'lap': 3795, 'pumpkins': 3796, 'pam': 3797, 'guests': 3798, 'squares': 3799, 'patted': 3800, 'spreading': 3801, 'bravery': 3802, 'davey': 3803, 'tearing': 3804, 'practicing': 3805, 'batteries': 3806, 'singer': 3807, 'crow': 3808, 'speaking': 3809, 'despite': 3810, 'rudy': 3811, 'losing': 3812, 'chirp': 3813, 'ranger': 3814, 'competition': 3815, 'sails': 3816, 'husband': 3817, 'vowed': 3818, 'tweetie': 3819, 'firefighters': 3820, 'caw': 3821, 'megan': 3822, 'calmed': 3823, 'minerals': 3824, 'decorating': 3825, 'comforted': 3826, 'mommies': 3827, 'pouring': 3828, 'grip': 3829, 'aside': 3830, 'unpacked': 3831, 'rolls': 3832, 'workers': 3833, 'planning': 3834, 'grandfather': 3835, 'rip': 3836, 'bathing': 3837, 'peel': 3838, 'liam': 3839, 'stroked': 3840, 'rotting': 3841, 'claire': 3842, 'filling': 3843, 'ruff': 3844, 'vivi': 3845, 'flip': 3846, 'softer': 3847, 'snowmen': 3848, 'tippy': 3849, 'dentist': 3850, 'marked': 3851, 'wires': 3852, 'entrance': 3853, 'twinkling': 3854, 'sprang': 3855, 'remembering': 3856, 'steady': 3857, 'scratches': 3858, 'surfing': 3859, 'players': 3860, 'counts': 3861, 'knees': 3862, 'refreshed': 3863, 'ripe': 3864, 'lighter': 3865, 'uses': 3866, 'reflection': 3867, '“let': 3868, 'shorts': 3869, 'arrows': 3870, 'necklaces': 3871, 'wandering': 3872, 'sale': 3873, 'paddle': 3874, 'surface': 3875, 'chefs': 3876, 'flames': 3877, 'jackie': 3878, 'ziggy': 3879, 'raccoon': 3880, 'beth': 3881, 'mood': 3882, 'exhausted': 3883, 'passing': 3884, 'knowledge': 3885, 'b': 4017, '“come': 3887, 'included': 3888, 'drums': 3889, 'stem': 3890, 'outdoors': 3891, 'recorder': 3892, 'third': 3893, 'closes': 3894, 'desert': 3895, 'single': 3896, 'minded': 3897, 'cheeks': 3898, 'shelves': 3899, 'choo': 3900, 'strongest': 3901, 'shovels': 3902, 'touches': 3903, 'performed': 3904, 'watering': 3905, 'tangled': 3906, 'relief': 3907, 'skill': 3908, 'chuckled': 3909, 'licking': 3910, 'bunch': 3911, 'seeker': 3912, 'hider': 3913, 'entire': 3914, 'colored': 3915, 'control': 3916, 'supported': 3917, 'rising': 3918, 'towels': 3919, 'flapping': 3920, 'least': 3921, 'lollipops': 3922, 'stool': 3923, 'winked': 3924, 'notes': 3925, 'softy': 3926, 'burns': 3927, 'student': 3928, 'brad': 3929, 'trampoline': 3930, 'prayer': 3931, 'poster': 3932, 'signed': 3933, 'waiter': 3934, 'pastries': 3935, 'ralph': 3936, 'starry': 3937, 'faye': 3938, 'trixie': 3939, 'suki': 3940, 'mat': 3941, 'oxy': 3942, 'matters': 3943, 'baths': 3944, 'whistles': 3945, 'ends': 3946, 'surroundings': 3947, '“mommy': 3948, 'fairies': 3949, 'impossible': 3950, 'swooped': 3951, 'squeaked': 3952, 'joining': 3953, 'tables': 3954, 'rusty': 3955, 'sewed': 3956, 'blown': 3957, 'what’s': 3958, 'flavor': 3959, 'lending': 3960, 'wipes': 3961, 'consequences': 3962, 'master': 3963, 'machines': 3964, 'twirling': 3965, 'talent': 3966, 'rains': 3967, 'fits': 3968, 'swords': 3969, 'clowns': 3970, 'bally': 3971, 'won’t': 3972, 'hesitated': 3973, 'introduced': 3974, 'ribbit': 3975, 'decisions': 3976, 'pops': 3977, 'mushrooms': 3978, 'michael': 3979, 'prayers': 3980, 'robin': 3981, 'albert': 3982, 'darkness': 3983, 'icing': 3984, 'sneezing': 3985, 'aimed': 3986, 'bars': 3987, 'ornaments': 3988, 'momma': 3989, 'dina': 3990, 'stew': 3991, 'dancers': 3992, 'loading': 3993, 'rocked': 3994, 'backpacks': 3995, 'turtles': 3996, 'adding': 3997, 'kittens': 3998, 'ships': 3999, 'frosty': 4000, 'toaster': 4001, 'jennie': 4002, 'cousin': 4003, 'jones': 4004, 'ghosts': 4005, 'r': 4007, '2': 4008, 'w': 4010, 'l': 4011, '\\n': 4012, 'q': 4013, '6': 4014, 'k': 4015, '0': 4016, 'u': 4018, '8': 4019, '1': 4020, '\\x93': 4022, '\\x92': 4023, '4': 4024, ' ': 4026, '=': 4028, 'g': 4031, '…': 4034, 'f': 4036, '9': 4039, '5': 4040, '“': 4041, 'c': 4043, '`': 4045, 'v': 4046, '‘': 4047, '7': 4048, 'z': 4052, 'e': 4053, 'ñ': 4054, 'é': 4057, 'o': 4058, 'j': 4059, '—': 4061, 'p': 4062, 'y': 4063, 'h': 4064, '/': 4065, 'n': 4066, '\\x94': 4067}\n",
      "[2, 339, 8, 583, 26, 4, 4066, 4036, 4064, 4042, 4031, 4008, 4038, 4020, 4028, 1, 4066, 4015, 4042, 4017, 4046, 4036, 4042, 5, 3024]\n",
      "hello , world ! nfhdg231=nkdbvfd sunday \n"
     ]
    }
   ],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, text_source):\n",
    "        self.text_source = text_source\n",
    "        self.max_word_vocab = 4000\n",
    "        self.special_tokens = {'<PAD>':int(0), '<UNKNOWN>':int(1), '<BOS>':int(2), '<EOS>':int(3), '<CHAR_START>':int(4), '<CHAR_END>':int(5)}\n",
    "        self.punctuation = r'[.,!?;:\"\\'\\-\\(\\)\\[\\]{}]'\n",
    "        self.word_to_id = {}\n",
    "        self.id_to_word = {}\n",
    "        self.char_to_id = {}\n",
    "        self.id_to_char = {}\n",
    "        self.vocab_len = 0\n",
    "\n",
    "\n",
    "    def build_vocab(self):\n",
    "        print (len(self.text_source))\n",
    "        grouped_stories = []\n",
    "        story = \"\"\n",
    "        for line in self.text_source.split(\"\\n\"):\n",
    "            if line == \"<|endoftext|>\":\n",
    "                # story+=\"\\n==============\\n\"\n",
    "                grouped_stories.append(story)   \n",
    "                story = \"\"\n",
    "            else:\n",
    "                story += line + \"\\n\"\n",
    "        all_words = []\n",
    "        all_chars = set()\n",
    "        for story in grouped_stories:\n",
    "            story = story.lower()\n",
    "            all_chars.update(story)\n",
    "\n",
    "            story = re.sub(self.punctuation, r' \\g<0> ', story) #Replace punctuation with space + punctuation + space\n",
    "            tokens = [token for token in story.split() if token.strip()]\n",
    "            all_words.extend(tokens)\n",
    "\n",
    "        \n",
    "        word_counts = Counter(all_words)\n",
    "        # print (len(all_words))\n",
    "        # print (word_counts)\n",
    "        start_id = len(self.special_tokens)\n",
    "        for word, count in word_counts.most_common(self.max_word_vocab):\n",
    "            self.word_to_id[word] = start_id\n",
    "            self.id_to_word[start_id] = word\n",
    "            start_id += 1\n",
    "        # print (self.word_to_id)\n",
    "        # print (self.id_to_word)\n",
    "        # print (all_chars)\n",
    "        for char in all_chars:\n",
    "            self.char_to_id[char] = start_id\n",
    "            self.id_to_char[start_id] = char\n",
    "            start_id += 1\n",
    "        \n",
    "        self.vocab_len = start_id\n",
    "        # print (self.char_to_id)\n",
    "        # print (self.id_to_char)\n",
    "    def encode(self, text, add_bos=True):\n",
    "        text = text.lower()\n",
    "        text = re.sub(self.punctuation, r' \\g<0> ', text)\n",
    "        # print (text)\n",
    "        tokens = [token for token in text.split() if token.strip()]\n",
    "        # print (tokens)\n",
    "        token_ids = []\n",
    "        token_ids.append(self.special_tokens['<BOS>'])\n",
    "        for token in tokens:\n",
    "            if token in self.word_to_id:\n",
    "                token_ids.append(self.word_to_id[token])\n",
    "            else:\n",
    "                token_ids.append(self.special_tokens['<CHAR_START>'])\n",
    "                for char in token:\n",
    "                    if char in self.char_to_id:\n",
    "                        token_ids.append(self.char_to_id[char])\n",
    "                    else:\n",
    "                        token_ids.append(self.special_tokens['<UNKNOWN>'])\n",
    "                token_ids.append(self.special_tokens['<CHAR_END>'])\n",
    "        return token_ids\n",
    "    def decode(self, token_ids):\n",
    "        decoded_text = \"\"\n",
    "        for token_id in token_ids:\n",
    "            if token_id > self.special_tokens['<CHAR_END>']:\n",
    "                if token_id in self.id_to_word:\n",
    "                    decoded_text += self.id_to_word[token_id] + \" \"\n",
    "                else:\n",
    "                    decoded_text += self.id_to_char[token_id]\n",
    "            if token_id == self.special_tokens['<CHAR_END>']:\n",
    "                decoded_text += \" \"\n",
    "        return decoded_text\n",
    "    def vocab(self):\n",
    "        vocab = self.special_tokens.copy()\n",
    "        vocab.update(self.word_to_id)\n",
    "        vocab.update(self.char_to_id)\n",
    "        return vocab\n",
    "\n",
    "\n",
    "\n",
    "simple_tokenizer = Tokenizer(stories_text)\n",
    "simple_tokenizer.build_vocab()\n",
    "\n",
    "print (simple_tokenizer.vocab())\n",
    "\n",
    "\n",
    "token_ids = simple_tokenizer.encode(\"Hello, world! nfhdg231=*nkdbvfd sunday\")\n",
    "print (token_ids)\n",
    "print (simple_tokenizer.decode(token_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e590db9a",
   "metadata": {},
   "source": [
    "## 4. Model Definition\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "Before we define the model architecture, let's set up the key hyperparameters. These values control the size and behavior of our model and the training process.\n",
    "\n",
    "*   `batch_size` (B): The number of independent sequences we process in parallel.\n",
    "*   `max_context` (T): The maximum number of tokens in a sequence that the model can look at to predict the next token. This is also called the context length or block size.\n",
    "*   `learning_rate`: The step size for our optimizer. A smaller learning rate can lead to more stable training but might be slower.\n",
    "*   `device`: We'll use a 'cuda' (GPU) device if available, as it significantly speeds up training. Otherwise, we'll fall back to 'cpu'.\n",
    "*   `n_embd` (C): The dimensionality of the token embeddings. Each token will be represented by a vector of this size.\n",
    "*   `n_head`: The number of attention heads in the multi-head attention mechanism.\n",
    "*   `n_layer`: The number of transformer blocks in our model. A deeper model (more layers) can learn more complex patterns but is also more computationally expensive.\n",
    "*   `dropout`: A regularization technique to prevent overfitting. It randomly sets a fraction of neuron activations to zero during training.\n",
    "*   `vocab_size`: The total size of our vocabulary, determined by our tokenizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80599132",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe9e51c",
   "metadata": {},
   "source": [
    "The **global positional encoding** commonly used in the original Transformer model (from *\"Attention is All You Need\"* by Vaswani et al.) is a **fixed** (non-learned) sinusoidal encoding. It adds positional information to token embeddings based on their absolute position in the sequence.\n",
    "\n",
    "Given a position \\( \\text{pos} \\in \\mathbb{N} \\) (starting from 0) and a dimension index \\( i \\in \\{0, 1, \\dots, d_{\\text{model}}-1\\} \\), the positional encoding \\( PE \\) for dimension \\( i \\) at position \\( \\text{pos} \\) is defined as:\n",
    "\n",
    "\\[\n",
    "PE(\\text{pos}, 2i) = \\sin\\left( \\frac{\\text{pos}}{10000^{2i / d_{\\text{model}}}} \\right)\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "PE(\\text{pos}, 2i+1) = \\cos\\left( \\frac{\\text{pos}}{10000^{2i / d_{\\text{model}}}} \\right)\n",
    "\\]\n",
    "\n",
    "### In matrix form:\n",
    "Let \\( \\mathbf{P} \\in \\mathbb{R}^{L \\times d_{\\text{model}}} \\) be the positional encoding matrix for a sequence of length \\( L \\). Then:\n",
    "\n",
    "\\[\n",
    "\\mathbf{P}_{p, j} =\n",
    "\\begin{cases}\n",
    "\\sin\\left( \\frac{p}{10000^{j / d_{\\text{model}}}} \\right) & \\text{if } j \\text{ is even} \\\\\n",
    "\\cos\\left( \\frac{p}{10000^{(j-1) / d_{\\text{model}}}} \\right) & \\text{if } j \\text{ is odd}\n",
    "\\end{cases}\n",
    "\\]\n",
    "\n",
    "where:\n",
    "- \\( p = 0, 1, \\dots, L-1 \\) is the position,\n",
    "- \\( j = 0, 1, \\dots, d_{\\text{model}}-1 \\) is the dimension.\n",
    "\n",
    "### Final input to Transformer:\n",
    "\\[\n",
    "\\text{Input Embedding} = \\text{Token Embedding} + \\text{Positional Encoding}\n",
    "\\]\n",
    "\n",
    "This encoding allows the model to capture relative and absolute position information via attention mechanisms.\n",
    "\n",
    "> **Note**: These are **not learned** — they are deterministic functions of position. This is the standard \"global\" positional encoding used in vanilla Transformers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d49fc242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Vocab size: 4068\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 24 # B\n",
    "max_context = 512 # T, max sequence length\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "n_embd = 192 # C\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "vocab_size = simple_tokenizer.vocab_len\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Vocab size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbe790b",
   "metadata": {},
   "source": [
    "## 5. Training the Model\n",
    "\n",
    "### Data Preparation for Training\n",
    "\n",
    "Before we can start training, we need to prepare the data in a format suitable for our model. Here's what the next cell does:\n",
    "\n",
    "1.  **Tokenize all stories**: It iterates through all the stories in the dataset and encodes them into token IDs using our `simple_tokenizer`.\n",
    "2.  **Padding/Truncating**: Each story is padded with the `<EOS>` token ID or truncated to ensure all sequences have a length of `max_context`.\n",
    "3.  **Create a single tensor**: All the tokenized stories are concatenated into a single large tensor of token IDs.\n",
    "4.  **Reshape into batches**: The tensor is reshaped to have dimensions `(num_batches, batch_size, max_context)`. This creates batches of sequences that can be fed into the model.\n",
    "5.  **Create input and target data**: For language modeling, the model learns to predict the next token in a sequence. Therefore, `input_data` (`x`) is the sequence of tokens, and `target_data` (`y`) is the same sequence shifted by one position to the right. For example, if the input is \"the cat sat\", the target is \"cat sat on\". We prepare these two tensors for our training loop. We also add a padding token at the beginning of each sequence in `data` before splitting to facilitate this shifting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "754df155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# --- Positional Encoding ---\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=max_context):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is (B, T, C)\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "# --- Multi-Head Attention (Optimized) ---\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel - optimized version \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = head_size\n",
    "        self.n_embd = num_heads * head_size\n",
    "        \n",
    "        # Single linear layers for all heads combined\n",
    "        self.key = nn.Linear(self.n_embd, self.n_embd, bias=False)\n",
    "        self.query = nn.Linear(self.n_embd, self.n_embd, bias=False)\n",
    "        self.value = nn.Linear(self.n_embd, self.n_embd, bias=False)\n",
    "        \n",
    "        self.register_buffer('tril', torch.tril(torch.ones(max_context, max_context)))\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.proj = nn.Linear(self.n_embd, self.n_embd)\n",
    "        self.proj_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        # Compute Q, K, V for all heads at once\n",
    "        q = self.query(x)  # (B, T, n_embd)\n",
    "        k = self.key(x)    # (B, T, n_embd)\n",
    "        v = self.value(x)  # (B, T, n_embd)\n",
    "        \n",
    "        # Split into multiple heads: (B, T, n_embd) -> (B, T, num_heads, head_size) -> (B, num_heads, T, head_size)\n",
    "        q = q.view(B, T, self.num_heads, self.head_size).transpose(1, 2)\n",
    "        k = k.view(B, T, self.num_heads, self.head_size).transpose(1, 2)\n",
    "        v = v.view(B, T, self.num_heads, self.head_size).transpose(1, 2)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        att = (q @ k.transpose(-2, -1)) * (self.head_size ** -0.5)  # (B, num_heads, T, T)\n",
    "        att = att.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        out = att @ v  # (B, num_heads, T, head_size)\n",
    "        \n",
    "        # Concatenate heads: (B, num_heads, T, head_size) -> (B, T, num_heads, head_size) -> (B, T, n_embd)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, self.n_embd)\n",
    "        \n",
    "        # Final projection\n",
    "        out = self.proj_dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "# --- Feed Forward Network ---\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# --- Transformer Block ---\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class LanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_encoding = PositionalEncoding(n_embd, dropout, max_context)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        x = self.position_encoding(tok_emb)\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -max_context:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bddadf0",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "\n",
    "Now we'll set up the training loop.\n",
    "1.  We instantiate our `LanguageModel` and move it to the selected `device`. We also print the total number of parameters to get an idea of the model's size.\n",
    "2.  We create an `AdamW` optimizer, a popular choice for training transformer models.\n",
    "3.  We iterate through our prepared data, one batch at a time. For each batch:\n",
    "    *   We perform a **forward pass**: The model takes the input `xb` and `yb` and computes the logits and the loss (cross-entropy loss).\n",
    "    *   We perform a **backward pass**:\n",
    "        *   `optimizer.zero_grad()`: Clears old gradients from the previous step.\n",
    "        *   `loss.backward()`: Computes the gradients of the loss with respect to the model's parameters.\n",
    "        *   `optimizer.step()`: Updates the model's parameters using the computed gradients to minimize the loss.\n",
    "4.  We use `tqdm` to create a progress bar that shows the training progress and the current loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f9135ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14147072])\n",
      "torch.Size([1151, 24, 512])\n",
      "torch.Size([1151, 24, 513])\n",
      "torch.Size([1151, 24, 512])\n",
      "torch.Size([1151, 24, 512])\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the entire dataset\n",
    "all_token_ids = []\n",
    "stories = stories_text.split(\"<|endoftext|>\")\n",
    "eos_token_id = simple_tokenizer.special_tokens['<EOS>']\n",
    "for story in stories:\n",
    "    # print (story)\n",
    "    token_ids = simple_tokenizer.encode(story, add_bos=True)\n",
    "    if len(token_ids) > max_context:\n",
    "        token_ids = token_ids[:max_context]\n",
    "    else:\n",
    "        token_ids = token_ids + [eos_token_id] * (max_context - len(token_ids))\n",
    "    all_token_ids.extend(token_ids)\n",
    "    # print (len(token_ids))\n",
    "    # print (token_ids)\n",
    "data = torch.tensor(all_token_ids, dtype=torch.long)\n",
    "print (data.shape)\n",
    "\n",
    "\n",
    "data = data[:data.shape[0]//(batch_size*max_context)*(batch_size*max_context)]\n",
    "\n",
    "\n",
    "\n",
    "data = data.view(-1, batch_size, max_context)\n",
    "print (data.shape)\n",
    "pad = (torch.ones((data.shape[0], data.shape[1], 1), dtype=torch.long) * eos_token_id)\n",
    "data = torch.cat([pad, data], dim=-1).to(device)\n",
    "print (data.shape)\n",
    "\n",
    "\n",
    "\n",
    "input_data = data[:, :, :-1].contiguous()\n",
    "print (input_data.shape)\n",
    "target_data = data[:, :, 1:].contiguous()\n",
    "print (target_data.shape)\n",
    "\n",
    "# def get_batch(split):\n",
    "#     data = train_data if split == 'train' else val_data\n",
    "#     ix = torch.randint(len(data) - max_context, (batch_size,))\n",
    "#     x = torch.stack([data[i:i+max_context] for i in ix])\n",
    "#     y = torch.stack([data[i+1:i+max_context+1] for i in ix])\n",
    "#     x, y = x.to(device), y.to(device)\n",
    "#     return x, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d259ee07",
   "metadata": {},
   "source": [
    "## 6. Inference\n",
    "\n",
    "After training, let's use our model to generate some text. This process is often called inference.\n",
    "\n",
    "1.  We provide a `prompt` to give the model a starting point.\n",
    "2.  The prompt is tokenized and converted into a tensor.\n",
    "3.  We use `torch.no_grad()` to ensure that no gradients are computed, which makes the process more efficient as we are not training.\n",
    "4.  We call the `model.generate()` method, which takes the context and generates `max_new_tokens` tokens.\n",
    "5.  Finally, we decode the generated token IDs back into text to see what our model has learned to write! The output will be a short story that starts with our prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e355e62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "print (stories[random.randint(0, 1000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18897091",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ebbece5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.23M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1151/1151 [01:29<00:00, 12.81it/s, loss=1.46]\n",
      "Training: 100%|██████████| 1151/1151 [01:22<00:00, 13.95it/s, loss=1.3]  \n",
      "Training: 100%|██████████| 1151/1151 [01:22<00:00, 13.91it/s, loss=1.2]  \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "model = LanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(f\"{sum(p.numel() for p in m.parameters())/1e6:.2f}M parameters\")\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "for j in range(3):\n",
    "    training_loop = tqdm(range(input_data.shape[0]), desc=\"Training\")\n",
    "    for iter in training_loop:\n",
    "        # every once in a while evaluate the loss on train and val sets\n",
    "        # if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        #     losses = estimate_loss()\n",
    "        #     print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "        # t1 = time.time()\n",
    "        # sample a batch of data\n",
    "        xb, yb = input_data[iter], target_data[iter]\n",
    "        # t2 = time.time()\n",
    "        # print(f\"Time taken to get batch: {t2-t1:.2f} seconds\")\n",
    "        # t3 = time.time()\n",
    "        # evaluate the loss\n",
    "        logits, loss = model(xb, yb)\n",
    "        # t3 = time.time()\n",
    "        # print(f\"Time taken to get logits: {t3-t2:.2f} seconds\")\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # t4 = time.time()\n",
    "        # print(f\"Time taken to step: {t4-t3:.2f} seconds\")\n",
    "        training_loop.set_postfix(loss=loss.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde29b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_accum = []\n",
    "for j in range (3):\n",
    "    training_loop = tqdm(range(input_data.shape[0]), desc=\"Training\")\n",
    "    for iter in training_loop:\n",
    "        # every once in a while evaluate the loss on train and val sets\n",
    "        # if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        #     losses = estimate_loss()\n",
    "        #     print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "        # t1 = time.time()\n",
    "        # sample a batch of data\n",
    "        xb, yb = input_data[iter], target_data[iter]\n",
    "        # t2 = time.time()\n",
    "        # print(f\"Time taken to get batch: {t2-t1:.2f} seconds\")\n",
    "        # t3 = time.time()\n",
    "        # evaluate the loss\n",
    "        logits, loss = model(xb, yb)\n",
    "        # t3 = time.time()\n",
    "        # print(f\"Time taken to get logits: {t3-t2:.2f} seconds\")\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # t4 = time.time()\n",
    "        # print(f\"Time taken to step: {t4-t3:.2f} seconds\")\n",
    "        training_loop.set_postfix(loss=loss.item())\n",
    "        loss_accum.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6815e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "once upon a time , in a wild place with big trees , there lived a small bunny in a tree . the bunny liked to make a lot of toys before bugs together . one day , a friend and a bird . the dog was surprised to make a fun in the park near the tree . they also wanted to see what happened . the \n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "prompt = \"One day, a blue bird named Billy\"\n",
    "prompt = \"Once upon a time, in a wild place with big trees, there lived a small bunny\"\n",
    "token_ids = simple_tokenizer.encode(prompt, add_bos=True)\n",
    "context = torch.tensor([token_ids], dtype=torch.long, device=device)\n",
    "with torch.no_grad():\n",
    "    generated_ids = m.generate(context, max_new_tokens=50)[0].tolist()\n",
    "    print(simple_tokenizer.decode(generated_ids))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad85ed4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c001e8f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
