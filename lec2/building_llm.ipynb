{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3984a680",
   "metadata": {},
   "source": [
    "# Building a Language Model from Scratch\n",
    "\n",
    "This notebook walks you through the process of building and training a small transformer-based language model from scratch using PyTorch. We will cover the following steps:\n",
    "1.  **Setting up the Environment**: Importing necessary libraries.\n",
    "2.  **Data Preparation**: Downloading and preprocessing the TinyStories dataset.\n",
    "3.  **Tokenization**: Creating a custom tokenizer to convert text into numerical representations.\n",
    "4.  **Model Definition**: Building the components of a transformer model (Positional Encoding, Multi-Head Attention, Feed-Forward Networks) and assembling them into a language model.\n",
    "5.  **Training**: Setting up the training loop to train the model on our dataset.\n",
    "6.  **Inference**: Using the trained model to generate new text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641a8572",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6c8a9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9640326",
   "metadata": {},
   "source": [
    "## 1. Setting up the Environment\n",
    "\n",
    "First, let's import the necessary libraries that we'll use throughout the notebook. These include `os` for interacting with the file system, `requests` for downloading data from the internet, `zipfile` for handling compressed files, `re` for regular expressions (useful for tokenization), and `Counter` for counting word frequencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1a75fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, requests, zipfile, re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655f24db",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "\n",
    "### Downloading the Dataset\n",
    "We'll be using the \"TinyStories\" dataset, which is a collection of short stories generated by GPT-3.5 and GPT-4 that are simple enough for a small model to learn from. The code below defines the URL for the dataset and a function to download it if it's not already present in our local directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4a80d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_url = \"https://github.com/entropicemergence/tiny_llm_server/releases/download/v0.1.0/TinyStoriesV2-GPT4-small.zip\"\n",
    "datset_folder = \"lec2/stories\"\n",
    "os.makedirs(datset_folder, exist_ok=True)\n",
    "\n",
    "def download_dataset(url):\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    filepath = os.path.join(datset_folder, filename)\n",
    "    if not os.path.exists(filepath):\n",
    "        response = requests.get(url)\n",
    "        with open(filepath, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "download_dataset(dataset_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b611b4",
   "metadata": {},
   "source": [
    "### Extracting and Loading the Data\n",
    "\n",
    "After downloading the compressed file, we need to extract its contents. The following cell unzips the file and then reads the entire text from the `.txt` file into a single string variable called `stories_text`. This variable will be our entire corpus for training the language model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903a11fb",
   "metadata": {},
   "source": [
    "## 3. Tokenization\n",
    "\n",
    "Tokenization is the process of converting a sequence of text into a sequence of numerical IDs that our model can understand. We'll create a custom `Tokenizer` class to handle this.\n",
    "\n",
    "Our tokenizer will use a hybrid approach:\n",
    "*   **Word-level Tokenization**: It will map the most common words in our vocabulary to unique integer IDs.\n",
    "*   **Character-level Tokenization**: For words that are not in our vocabulary (out-of-vocabulary or OOV words), it will break them down into individual characters and map those to IDs.\n",
    "\n",
    "This hybrid strategy allows us to handle any word we encounter while keeping the vocabulary size manageable.\n",
    "\n",
    "The tokenizer also includes several **special tokens**:\n",
    "*   `<PAD>`: Padding token, used to make all sequences in a batch the same length.\n",
    "*   `<UNKNOWN>`: Used for characters that are not in our character vocabulary.\n",
    "*   `<BOS>`: \"Beginning of Sequence\" token, marks the start of a text sequence.\n",
    "*   `<EOS>`: \"End of Sequence\" token, marks the end of a text sequence.\n",
    "*   `<CHAR_START>` / `<CHAR_END>`: These tokens signal the beginning and end of a sequence of characters representing an OOV word.\n",
    "\n",
    "The cell below defines the `Tokenizer` class, builds the vocabulary from our `stories_text`, and then demonstrates how to encode a sample sentence into token IDs and decode it back into text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538a9d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"lec2/stories/TinyStoriesV2-GPT4-small.zip\"\n",
    "with zipfile.ZipFile(filepath, \"r\") as zip_ref:\n",
    "    zip_ref.extractall(datset_folder)\n",
    "with open(\"lec2/stories/TinyStoriesV2-GPT4-small.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    stories_text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b1e36a",
   "metadata": {
    "vscode": {
     "languageId": "markdown\n"
    }
   },
   "source": [
    "Now, we'll import the core `PyTorch` libraries. `torch` is the main library, `torch.nn` provides building blocks for neural networks (like layers, activation functions, etc.), and `torch.optim` contains optimization algorithms like Adam.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e12a443",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, text_source):\n",
    "        self.text_source = text_source\n",
    "        self.max_word_vocab = 4000\n",
    "        self.special_tokens = {'<PAD>':int(0), '<UNKNOWN>':int(1), '<BOS>':int(2), '<EOS>':int(3), '<CHAR_START>':int(4), '<CHAR_END>':int(5)}\n",
    "        self.punctuation = r'[.,!?;:\"\\'\\-\\(\\)\\[\\]{}]'\n",
    "        self.word_to_id = {}\n",
    "        self.id_to_word = {}\n",
    "        self.char_to_id = {}\n",
    "        self.id_to_char = {}\n",
    "        self.vocab_len = 0\n",
    "\n",
    "\n",
    "    def build_vocab(self):\n",
    "        print (len(self.text_source))\n",
    "        grouped_stories = []\n",
    "        story = \"\"\n",
    "        for line in self.text_source.split(\"\\n\"):\n",
    "            if line == \"<|endoftext|>\":\n",
    "                # story+=\"\\n==============\\n\"\n",
    "                grouped_stories.append(story)   \n",
    "                story = \"\"\n",
    "            else:\n",
    "                story += line + \"\\n\"\n",
    "        all_words = []\n",
    "        all_chars = set()\n",
    "        for story in grouped_stories:\n",
    "            story = story.lower()\n",
    "            all_chars.update(story)\n",
    "\n",
    "            story = re.sub(self.punctuation, r' \\g<0> ', story) #Replace punctuation with space + punctuation + space\n",
    "            tokens = [token for token in story.split() if token.strip()]\n",
    "            all_words.extend(tokens)\n",
    "\n",
    "        \n",
    "        word_counts = Counter(all_words)\n",
    "        # print (len(all_words))\n",
    "        # print (word_counts)\n",
    "        start_id = len(self.special_tokens)\n",
    "        for word, count in word_counts.most_common(self.max_word_vocab):\n",
    "            self.word_to_id[word] = start_id\n",
    "            self.id_to_word[start_id] = word\n",
    "            start_id += 1\n",
    "        # print (self.word_to_id)\n",
    "        # print (self.id_to_word)\n",
    "        # print (all_chars)\n",
    "        for char in all_chars:\n",
    "            self.char_to_id[char] = start_id\n",
    "            self.id_to_char[start_id] = char\n",
    "            start_id += 1\n",
    "        \n",
    "        self.vocab_len = start_id\n",
    "        # print (self.char_to_id)\n",
    "        # print (self.id_to_char)\n",
    "    def encode(self, text, add_bos=True):\n",
    "        text = text.lower()\n",
    "        text = re.sub(self.punctuation, r' \\g<0> ', text)\n",
    "        # print (text)\n",
    "        tokens = [token for token in text.split() if token.strip()]\n",
    "        # print (tokens)\n",
    "        token_ids = []\n",
    "        token_ids.append(self.special_tokens['<BOS>'])\n",
    "        for token in tokens:\n",
    "            if token in self.word_to_id:\n",
    "                token_ids.append(self.word_to_id[token])\n",
    "            else:\n",
    "                token_ids.append(self.special_tokens['<CHAR_START>'])\n",
    "                for char in token:\n",
    "                    if char in self.char_to_id:\n",
    "                        token_ids.append(self.char_to_id[char])\n",
    "                    else:\n",
    "                        token_ids.append(self.special_tokens['<UNKNOWN>'])\n",
    "                token_ids.append(self.special_tokens['<CHAR_END>'])\n",
    "        return token_ids\n",
    "    def decode(self, token_ids):\n",
    "        decoded_text = \"\"\n",
    "        for token_id in token_ids:\n",
    "            if token_id > self.special_tokens['<CHAR_END>']:\n",
    "                if token_id in self.id_to_word:\n",
    "                    decoded_text += self.id_to_word[token_id] + \" \"\n",
    "                else:\n",
    "                    decoded_text += self.id_to_char[token_id]\n",
    "            if token_id == self.special_tokens['<CHAR_END>']:\n",
    "                decoded_text += \" \"\n",
    "        return decoded_text\n",
    "    def vocab(self):\n",
    "        vocab = self.special_tokens.copy()\n",
    "        vocab.update(self.word_to_id)\n",
    "        vocab.update(self.char_to_id)\n",
    "        return vocab\n",
    "\n",
    "\n",
    "\n",
    "simple_tokenizer = Tokenizer(stories_text)\n",
    "simple_tokenizer.build_vocab()\n",
    "\n",
    "print (simple_tokenizer.vocab())\n",
    "\n",
    "\n",
    "token_ids = simple_tokenizer.encode(\"Hello, world! nfhdg231=*nkdbvfd sunday\")\n",
    "print (token_ids)\n",
    "print (simple_tokenizer.decode(token_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e590db9a",
   "metadata": {},
   "source": [
    "## 4. Model Definition\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "Before we define the model architecture, let's set up the key hyperparameters. These values control the size and behavior of our model and the training process.\n",
    "\n",
    "*   `batch_size` (B): The number of independent sequences we process in parallel.\n",
    "*   `max_context` (T): The maximum number of tokens in a sequence that the model can look at to predict the next token. This is also called the context length or block size.\n",
    "*   `learning_rate`: The step size for our optimizer. A smaller learning rate can lead to more stable training but might be slower.\n",
    "*   `device`: We'll use a 'cuda' (GPU) device if available, as it significantly speeds up training. Otherwise, we'll fall back to 'cpu'.\n",
    "*   `n_embd` (C): The dimensionality of the token embeddings. Each token will be represented by a vector of this size.\n",
    "*   `n_head`: The number of attention heads in the multi-head attention mechanism.\n",
    "*   `n_layer`: The number of transformer blocks in our model. A deeper model (more layers) can learn more complex patterns but is also more computationally expensive.\n",
    "*   `dropout`: A regularization technique to prevent overfitting. It randomly sets a fraction of neuron activations to zero during training.\n",
    "*   `vocab_size`: The total size of our vocabulary, determined by our tokenizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80599132",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe9e51c",
   "metadata": {},
   "source": [
    "The **global positional encoding** commonly used in the original Transformer model (from *\"Attention is All You Need\"* by Vaswani et al.) is a **fixed** (non-learned) sinusoidal encoding. It adds positional information to token embeddings based on their absolute position in the sequence.\n",
    "\n",
    "Given a position \\( \\text{pos} \\in \\mathbb{N} \\) (starting from 0) and a dimension index \\( i \\in \\{0, 1, \\dots, d_{\\text{model}}-1\\} \\), the positional encoding \\( PE \\) for dimension \\( i \\) at position \\( \\text{pos} \\) is defined as:\n",
    "\n",
    "\\[\n",
    "PE(\\text{pos}, 2i) = \\sin\\left( \\frac{\\text{pos}}{10000^{2i / d_{\\text{model}}}} \\right)\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "PE(\\text{pos}, 2i+1) = \\cos\\left( \\frac{\\text{pos}}{10000^{2i / d_{\\text{model}}}} \\right)\n",
    "\\]\n",
    "\n",
    "### In matrix form:\n",
    "Let \\( \\mathbf{P} \\in \\mathbb{R}^{L \\times d_{\\text{model}}} \\) be the positional encoding matrix for a sequence of length \\( L \\). Then:\n",
    "\n",
    "\\[\n",
    "\\mathbf{P}_{p, j} =\n",
    "\\begin{cases}\n",
    "\\sin\\left( \\frac{p}{10000^{j / d_{\\text{model}}}} \\right) & \\text{if } j \\text{ is even} \\\\\n",
    "\\cos\\left( \\frac{p}{10000^{(j-1) / d_{\\text{model}}}} \\right) & \\text{if } j \\text{ is odd}\n",
    "\\end{cases}\n",
    "\\]\n",
    "\n",
    "where:\n",
    "- \\( p = 0, 1, \\dots, L-1 \\) is the position,\n",
    "- \\( j = 0, 1, \\dots, d_{\\text{model}}-1 \\) is the dimension.\n",
    "\n",
    "### Final input to Transformer:\n",
    "\\[\n",
    "\\text{Input Embedding} = \\text{Token Embedding} + \\text{Positional Encoding}\n",
    "\\]\n",
    "\n",
    "This encoding allows the model to capture relative and absolute position information via attention mechanisms.\n",
    "\n",
    "> **Note**: These are **not learned** â€” they are deterministic functions of position. This is the standard \"global\" positional encoding used in vanilla Transformers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49fc242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 24 # B\n",
    "max_context = 512 # T, max sequence length\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "n_embd = 192 # C\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "vocab_size = simple_tokenizer.vocab_len\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Vocab size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbe790b",
   "metadata": {},
   "source": [
    "## 5. Training the Model\n",
    "\n",
    "### Data Preparation for Training\n",
    "\n",
    "Before we can start training, we need to prepare the data in a format suitable for our model. Here's what the next cell does:\n",
    "\n",
    "1.  **Tokenize all stories**: It iterates through all the stories in the dataset and encodes them into token IDs using our `simple_tokenizer`.\n",
    "2.  **Padding/Truncating**: Each story is padded with the `<EOS>` token ID or truncated to ensure all sequences have a length of `max_context`.\n",
    "3.  **Create a single tensor**: All the tokenized stories are concatenated into a single large tensor of token IDs.\n",
    "4.  **Reshape into batches**: The tensor is reshaped to have dimensions `(num_batches, batch_size, max_context)`. This creates batches of sequences that can be fed into the model.\n",
    "5.  **Create input and target data**: For language modeling, the model learns to predict the next token in a sequence. Therefore, `input_data` (`x`) is the sequence of tokens, and `target_data` (`y`) is the same sequence shifted by one position to the right. For example, if the input is \"the cat sat\", the target is \"cat sat on\". We prepare these two tensors for our training loop. We also add a padding token at the beginning of each sequence in `data` before splitting to facilitate this shifting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754df155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# --- Positional Encoding ---\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=max_context):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is (B, T, C)\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "# --- Multi-Head Attention (Optimized) ---\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel - optimized version \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = head_size\n",
    "        self.n_embd = num_heads * head_size\n",
    "        \n",
    "        # Single linear layers for all heads combined\n",
    "        self.key = nn.Linear(self.n_embd, self.n_embd, bias=False)\n",
    "        self.query = nn.Linear(self.n_embd, self.n_embd, bias=False)\n",
    "        self.value = nn.Linear(self.n_embd, self.n_embd, bias=False)\n",
    "        \n",
    "        self.register_buffer('tril', torch.tril(torch.ones(max_context, max_context)))\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.proj = nn.Linear(self.n_embd, self.n_embd)\n",
    "        self.proj_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        # Compute Q, K, V for all heads at once\n",
    "        q = self.query(x)  # (B, T, n_embd)\n",
    "        k = self.key(x)    # (B, T, n_embd)\n",
    "        v = self.value(x)  # (B, T, n_embd)\n",
    "        \n",
    "        # Split into multiple heads: (B, T, n_embd) -> (B, T, num_heads, head_size) -> (B, num_heads, T, head_size)\n",
    "        q = q.view(B, T, self.num_heads, self.head_size).transpose(1, 2)\n",
    "        k = k.view(B, T, self.num_heads, self.head_size).transpose(1, 2)\n",
    "        v = v.view(B, T, self.num_heads, self.head_size).transpose(1, 2)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        att = (q @ k.transpose(-2, -1)) * (self.head_size ** -0.5)  # (B, num_heads, T, T)\n",
    "        att = att.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        out = att @ v  # (B, num_heads, T, head_size)\n",
    "        \n",
    "        # Concatenate heads: (B, num_heads, T, head_size) -> (B, T, num_heads, head_size) -> (B, T, n_embd)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, self.n_embd)\n",
    "        \n",
    "        # Final projection\n",
    "        out = self.proj_dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "# --- Feed Forward Network ---\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# --- Transformer Block ---\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class LanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_encoding = PositionalEncoding(n_embd, dropout, max_context)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        x = self.position_encoding(tok_emb)\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -max_context:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bddadf0",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "\n",
    "Now we'll set up the training loop.\n",
    "1.  We instantiate our `LanguageModel` and move it to the selected `device`. We also print the total number of parameters to get an idea of the model's size.\n",
    "2.  We create an `AdamW` optimizer, a popular choice for training transformer models.\n",
    "3.  We iterate through our prepared data, one batch at a time. For each batch:\n",
    "    *   We perform a **forward pass**: The model takes the input `xb` and `yb` and computes the logits and the loss (cross-entropy loss).\n",
    "    *   We perform a **backward pass**:\n",
    "        *   `optimizer.zero_grad()`: Clears old gradients from the previous step.\n",
    "        *   `loss.backward()`: Computes the gradients of the loss with respect to the model's parameters.\n",
    "        *   `optimizer.step()`: Updates the model's parameters using the computed gradients to minimize the loss.\n",
    "4.  We use `tqdm` to create a progress bar that shows the training progress and the current loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9135ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the entire dataset\n",
    "all_token_ids = []\n",
    "stories = stories_text.split(\"<|endoftext|>\")\n",
    "eos_token_id = simple_tokenizer.special_tokens['<EOS>']\n",
    "for story in stories:\n",
    "    # print (story)\n",
    "    token_ids = simple_tokenizer.encode(story, add_bos=True)\n",
    "    if len(token_ids) > max_context:\n",
    "        token_ids = token_ids[:max_context]\n",
    "    else:\n",
    "        token_ids = token_ids + [eos_token_id] * (max_context - len(token_ids))\n",
    "    all_token_ids.extend(token_ids)\n",
    "    # print (len(token_ids))\n",
    "    # print (token_ids)\n",
    "data = torch.tensor(all_token_ids, dtype=torch.long)\n",
    "print (data.shape)\n",
    "\n",
    "\n",
    "data = data[:data.shape[0]//(batch_size*max_context)*(batch_size*max_context)]\n",
    "\n",
    "\n",
    "\n",
    "data = data.view(-1, batch_size, max_context)\n",
    "print (data.shape)\n",
    "pad = (torch.ones((data.shape[0], data.shape[1], 1), dtype=torch.long) * eos_token_id)\n",
    "data = torch.cat([pad, data], dim=-1).to(device)\n",
    "print (data.shape)\n",
    "\n",
    "\n",
    "\n",
    "input_data = data[:, :, :-1].contiguous()\n",
    "print (input_data.shape)\n",
    "target_data = data[:, :, 1:].contiguous()\n",
    "print (target_data.shape)\n",
    "\n",
    "# def get_batch(split):\n",
    "#     data = train_data if split == 'train' else val_data\n",
    "#     ix = torch.randint(len(data) - max_context, (batch_size,))\n",
    "#     x = torch.stack([data[i:i+max_context] for i in ix])\n",
    "#     y = torch.stack([data[i+1:i+max_context+1] for i in ix])\n",
    "#     x, y = x.to(device), y.to(device)\n",
    "#     return x, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d259ee07",
   "metadata": {},
   "source": [
    "## 6. Inference\n",
    "\n",
    "After training, let's use our model to generate some text. This process is often called inference.\n",
    "\n",
    "1.  We provide a `prompt` to give the model a starting point.\n",
    "2.  The prompt is tokenized and converted into a tensor.\n",
    "3.  We use `torch.no_grad()` to ensure that no gradients are computed, which makes the process more efficient as we are not training.\n",
    "4.  We call the `model.generate()` method, which takes the context and generates `max_new_tokens` tokens.\n",
    "5.  Finally, we decode the generated token IDs back into text to see what our model has learned to write! The output will be a short story that starts with our prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e355e62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "print (stories[random.randint(0, 1000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18897091",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebbece5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "model = LanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(f\"{sum(p.numel() for p in m.parameters())/1e6:.2f}M parameters\")\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "# for j in range(10):\n",
    "\n",
    "training_loop = tqdm(range(input_data.shape[0]), desc=\"Training\")\n",
    "for iter in training_loop:\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    # if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "    #     losses = estimate_loss()\n",
    "    #     print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # t1 = time.time()\n",
    "    # sample a batch of data\n",
    "    xb, yb = input_data[iter], target_data[iter]\n",
    "    # t2 = time.time()\n",
    "    # print(f\"Time taken to get batch: {t2-t1:.2f} seconds\")\n",
    "    # t3 = time.time()\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    # t3 = time.time()\n",
    "    # print(f\"Time taken to get logits: {t3-t2:.2f} seconds\")\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # t4 = time.time()\n",
    "    # print(f\"Time taken to step: {t4-t3:.2f} seconds\")\n",
    "    training_loop.set_postfix(loss=loss.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde29b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_accum = []\n",
    "for j in range (3):\n",
    "    training_loop = tqdm(range(input_data.shape[0]), desc=\"Training\")\n",
    "    for iter in training_loop:\n",
    "        # every once in a while evaluate the loss on train and val sets\n",
    "        # if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        #     losses = estimate_loss()\n",
    "        #     print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "        # t1 = time.time()\n",
    "        # sample a batch of data\n",
    "        xb, yb = input_data[iter], target_data[iter]\n",
    "        # t2 = time.time()\n",
    "        # print(f\"Time taken to get batch: {t2-t1:.2f} seconds\")\n",
    "        # t3 = time.time()\n",
    "        # evaluate the loss\n",
    "        logits, loss = model(xb, yb)\n",
    "        # t3 = time.time()\n",
    "        # print(f\"Time taken to get logits: {t3-t2:.2f} seconds\")\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # t4 = time.time()\n",
    "        # print(f\"Time taken to step: {t4-t3:.2f} seconds\")\n",
    "        training_loop.set_postfix(loss=loss.item())\n",
    "        loss_accum.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6815e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate from the model\n",
    "prompt = \"One day, a blue bird named Billy\"\n",
    "prompt = \"Once upon a time, in a wild place with big trees, there lived a small bunny\"\n",
    "token_ids = simple_tokenizer.encode(prompt, add_bos=True)\n",
    "context = torch.tensor([token_ids], dtype=torch.long, device=device)\n",
    "with torch.no_grad():\n",
    "    generated_ids = m.generate(context, max_new_tokens=50)[0].tolist()\n",
    "    print(simple_tokenizer.decode(generated_ids))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad85ed4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c001e8f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
